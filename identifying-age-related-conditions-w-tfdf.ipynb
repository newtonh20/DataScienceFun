{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ICR - Identifying Age-Related Conditions Dataset with Capsule Neural Networks","metadata":{}},{"cell_type":"markdown","source":"This notebook aims to address the task of identifying age-related conditions using Capsule Neural Networks (CapsNet) specifically tuned for binary classification. Age-related conditions pose significant challenges in healthcare, and early detection plays a crucial role in improving patient outcomes and quality of life.\n\nCapsNet is a deep learning architecture known for its ability to capture hierarchical relationships among features, making it suitable for complex pattern recognition tasks. By leveraging the unique properties of CapsNet, we aim to develop a robust model that can accurately classify individuals into two categories: those affected by age-related conditions and those without such conditions.\n\nThe dataset used in this notebook consists of diverse features, including demographic information, medical history, and diagnostic tests. Through a series of data preprocessing steps, we will ensure that the data is appropriately prepared for training and evaluation. Additionally, we will employ k-fold cross-validation to assess the model's performance on multiple subsets of the data, enhancing its generalization capabilities.\n\nDuring model training, we will monitor key metrics such as accuracy and loss to evaluate the model's performance and identify potential overfitting or underfitting issues. Visualizations, including loss curves and accuracy plots, will be utilized to gain insights into the model's learning process and its behavior across different epochs.\n\nUpon selecting the best-performing model based on accuracy and loss, we will further evaluate its performance on a holdout test set to estimate its real-world predictive capabilities. Additionally, we will interpret the model's predictions using techniques like feature importance analysis and explainability tools to gain insights into the factors influencing the classification decisions.\n\nBy leveraging CapsNet's unique architecture and employing a rigorous evaluation framework, this notebook aims to contribute to the identification of age-related conditions, enabling early intervention and improved healthcare outcomes. The findings and insights derived from this work can potentially assist healthcare professionals in making informed decisions and provide valuable support in managing age-related conditions.","metadata":{}},{"cell_type":"markdown","source":"# Import the libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow_datasets as tfds\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport time\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"TensorFlow v\" + tf.__version__)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the Dataset","metadata":{}},{"cell_type":"code","source":"dataset_df = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\ndataset_df = dataset_df.set_index(\"Id\")\n# Remove leading or trailing whitespace from column names\ndataset_df.columns = dataset_df.columns.str.strip()\n\nprint(\"Full train dataset shape is {}\".format(dataset_df.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is composed of 58 columns and 617 entries. We can see all 58 dimensions(results will be truncated since the number of columns is big) of our dataset by printing out the first 5 entries using the following code:","metadata":{}},{"cell_type":"code","source":"dataset_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`Class` is the label column indicating if a person has one or more of any of the three medical conditions (i.e,`Class 1`), or none of the three medical conditions (i.e,`Class 0`).\nGiven the features of the dataset, the goal of our model is to predict the value of `Class` for any person.","metadata":{}},{"cell_type":"markdown","source":"# Quick basic dataset exploration","metadata":{}},{"cell_type":"code","source":"dataset_df.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"dataset_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-06-05T08:39:29.187353Z","iopub.execute_input":"2023-06-05T08:39:29.187811Z","iopub.status.idle":"2023-06-05T08:39:29.214705Z","shell.execute_reply.started":"2023-06-05T08:39:29.187771Z","shell.execute_reply":"2023-06-05T08:39:29.213636Z"}}},{"cell_type":"markdown","source":"## Pie chart for label column: Class","metadata":{}},{"cell_type":"code","source":"plot_df = dataset_df.Class.value_counts()\nplot_df.plot(kind=\"pie\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Important**: From the pie chart we can see that the dataset is heavily imbalanced since the fraction of positive(`1`) samples is very small compared to the negative(`0`) samples.","metadata":{}},{"cell_type":"markdown","source":"# Numerical data distribution\n\nFirst, we will list all the numerical columns names.","metadata":{}},{"cell_type":"code","source":"# Store all the numerical column names into a list\nNUM_FEATURE_COLUMNS = [i for i in dataset_df.columns if i not in [\"Id\", \"EJ\", \"Class\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us now plot the first 6 numerical columns and their values using bar charts.","metadata":{}},{"cell_type":"code","source":"figure, axis = plt.subplots(3, 2, figsize=(15, 15))\nplt.subplots_adjust(hspace=0.25, wspace=0.3)\n\nfor i, column_name in enumerate(NUM_FEATURE_COLUMNS[:6]):\n    row = i//2\n    col = i % 2\n    bp = sns.barplot(ax=axis[row, col], x=dataset_df.index, y=dataset_df[column_name])\n    bp.set(xticklabels=[])\n    axis[row, col].set_title(column_name)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will also create a list of feature columns that will be used for training. We will drop `Id` from the list since it is not needed.","metadata":{}},{"cell_type":"code","source":"FEATURE_COLUMNS = dataset_df.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let us split the dataset into training and testing datasets:","metadata":{}},{"cell_type":"markdown","source":"# KFold validation\n\nWe will use Stratified KFold cross validation for training this model since the normal train/test split training won't be enough to acheive decent score.\n\nStratified KFold cross-validation is a powerful technique used in machine learning to evaluate the performance of models, particularly when dealing with imbalanced datasets. It ensures that the distribution of target classes remains consistent across different folds during the validation process.\n\nIt is crucial to maintain the proportional representation of our classes in each fold of the cross-validation process. Stratified KFold cross-validation accomplishes this by dividing the dataset into a predetermined number of folds while preserving the relative class distribution in each fold. This technique guarantees that each fold accurately represents the distribution of the target classes found in the overall dataset.\n\nWhy is this important? Well, consider a scenario where one age group or condition is significantly underrepresented in the data. If we were to use a regular KFold cross-validation, we might end up with some folds completely lacking instances of that particular age group or condition. This could severely bias our model's performance evaluation, leading to misleading results.\n\nBy employing Stratified KFold cross-validation, we can ensure that each fold contains a representative sample of each age group or condition. This approach helps us obtain a more reliable estimation of our model's generalization performance, enabling us to make better-informed decisions about its effectiveness in predicting age-related conditions.\n\nWe will split the dataset into 10 consecutive folds. Each fold is then used once as a validation set while the 9 (10-1) remaining folds form the training set.\n\nRead more about KFold [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html).","metadata":{}},{"cell_type":"code","source":"# Creates a GroupKFold with 5 splits\nkf = StratifiedKFold(n_splits=10, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Out of Fold (OOF)\n\nDuring KFold cross validation, the predictions made on the test set of each fold is known as Out of Fold(OOF) predictions. We will estimate the performance of the model using the predictions made across all the k (5 in this example) folds.\n\nFor our training loop, we will create a pandas dataframe named `oof` to store the predictions of the validation set during each fold.","metadata":{}},{"cell_type":"code","source":"# Create list of ids for the creation of oof dataframe.\nID_LIST = dataset_df.index\n\n# Create a dataframe of required size with zero values.\noof = pd.DataFrame(data=np.zeros((len(ID_LIST),1)), index=ID_LIST)\n\n# Create an empty dictionary to store the models trained for each fold.\nmodels = {}\n\n# Create empty dict to save metircs for the models trained for each fold.\naccuracy = {}\nmargin_loss_dict = {}\n\n# Save the name of the label column to a variable.\nlabel = \"Class\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Strategies to handle the dataset imbalance\n\nLet's examine the fraction of positive and negative samples in this dataset's  by examining the`Class` column.","metadata":{}},{"cell_type":"code","source":"# Calculate the number of negative and positive values in `Class` column\nneg, pos = np.bincount(dataset_df['Class'])\n# Calculate total samples\ntotal = neg + pos\nprint('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n    total, pos, 100 * pos / total))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, positive samples only account for 17.50% of the data. This means that our datastet is heavily imbalanced.\n\nIn classification problems with imbalanced datasets, a model tends to be more biased towards the majority class leading to the model performing poorly on the samples belonging to the minority class.","metadata":{}},{"cell_type":"markdown","source":"There are many techniques for dealing with imbalanced data. The most suitable techniques for this dataset are:\n\n* Undersampling\n* Class weighting\n\nIn this notebook we will use **Class weighting** to deal with imbalanced data. But first, we will quickly outline how undersampling can be performed.","metadata":{}},{"cell_type":"markdown","source":"# Undersampling\n\nOne approach to deal with an imbalanced dataset would be to under-sample the majority class(es) by choosing a smaller subset of the majority class samples(negative or `0` class in this case) from the dataset rather than picking the entire data.\n\nNote: You have to loop through the dataset and try different random subsets for a better score.\n\nThe code snippet below illustrates how to perform undersampling.\n\n```\n# This function generates undersampled dataset.\ndef random_under_sampler(df):\n    # Calculate the number of samples for each label. \n    neg, pos = np.bincount(df['Class'])\n\n    # Choose the samples with class label `1`.\n    one_df = df.loc[df['Class'] == 1] \n    # Choose the samples with class label `0`.\n    zero_df = df.loc[df['Class'] == 0]\n    # Select `pos` number of negative samples.\n    # This makes sure that we have equal number of samples for each label.\n    zero_df = zero_df.sample(n=pos)\n\n    # Join both label dataframes.\n    undersampled_df = pd.concat([zero_df, one_df])\n\n    # Shuffle the data and return\n    return undersampled_df.sample(frac = 1)\n```","metadata":{}},{"cell_type":"markdown","source":"# Class weighting\n\nSince the postive(`1`) `Class` labels are only a small fraction of the dataset, we would want the classifier to heavily weight those examples. You can do this by passing **Keras weights** for each class through a parameter. This will cause the model to \"pay more attention\" to examples from an under-represented class. Read more about class weights [here](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights).","metadata":{}},{"cell_type":"code","source":"# Calculate the number of samples for each label.\nneg, pos = np.bincount(dataset_df['Class'])\n\n# Calculate total samples.\ntotal = neg + pos\n\n# Calculate the weight for each label.\nweight_for_0 = (1 / neg) * (total / 2.0)\nweight_for_1 = (1 / pos) * (total / 2.0)\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To train and evaluate the models using class weights, use the dict in model.fit() as an argument as shown below.\n\n```model.fit(x=train_ds, class_weight=class_weight)```","metadata":{}},{"cell_type":"markdown","source":"# Train Capsule Network Model\n\nA Capsule Network is a type of neural network that consists of groups of neurons called capsules. Each capsule represents a specific feature or entity and can encode its presence and pose (orientation, position, size, etc.) in a vector. The capsules are arranged in layers and communicate with each other through a dynamic routing mechanism that allows them to form part-whole relationships and preserve the spatial hierarchy of the input. A Capsule Network can learn to recognize complex patterns and generalize better than traditional convolutional neural networks, especially for tasks that require viewpoint invariance and pose estimation.\n\nIn the context of using Capsule Networks for data like illness diagnoses, the idea of capsules representing specific features or entities can still be applicable. Instead of detecting low-level visual features (edges, textures, etc.) as in image-based Capsule Networks, the capsules would now represent different attributes or characteristics related to the illness and the individual's age. Each capsule would encode the presence and pose of a specific attribute in the input vector.\n\nFor example, let's say we have a dataset with vectors containing various illness diagnoses and age information for different individuals. We could design the Capsule Network to have capsules that correspond to specific illnesses or health conditions and capsules that represent age-related features.\n\nThe dynamic routing mechanism would allow the capsules to interact with each other and form relationships between different attributes, capturing dependencies and correlations between illnesses and age-related features. The network could potentially learn to recognize complex patterns in the data, infer interactions between illnesses and age, and generalize better in predicting health outcomes for new individuals based on their diagnoses and age.\n\nThe Capsule Network model will consist of three main layers: an input layer, a primary capsule layer, and a digit capsule layer. The input layer will transform the input vector into a matrix of feature maps using convolutional filters. The primary capsule layer will apply another set of convolutional filters to the feature maps and reshape them into capsules. The digit capsule layer will use a dynamic routing algorithm to compute the activation and pose vectors for each of the four classes (three conditions and one negative class). The activation vector represents the probability that the class is present in the input, while the pose vector represents the parameters of the class. The model will use a margin loss function to optimize the activation vectors and a reconstruction loss function to optimize the pose vectors. The reconstruction loss function will use a decoder network to reconstruct the input vector from the pose vector of the predicted class and compare it with the original input vector.\n\n\n![Capsule Network diagram](https://www.researchgate.net/publication/369256529/figure/fig1/AS:11431281126975957@1678898063434/Structure-of-the-capsule-network_Q320.jpg)\n\nIn this project, I will use a Capsule Network to predict if a person has any of three medical conditions: diabetes, hypertension, or heart disease. The input data consists of measurements of health characteristics such as blood pressure, blood sugar, cholesterol, body mass index, etc. The output is a binary label indicating if the person has one or more of the three conditions (Class 1) or none of them (Class 0). I will train the Capsule Network on a balanced version of the dataset.\n\nBy default the model is set to train for a classification task. The margin loss function calculates the loss based on the predicted output y_pred and the true labels y_true. It incorporates margin values for positive and negative examples, as well as a down-weighting parameter for the absent class.\n\nMargin loss is a function that measures the discrepancy between the length of the output vector of a capsule network and a target value. For a binary classification task, the target value is either 0 or 1, depending on the class label. The margin loss can be written as:\n\n$$L = T \\cdot \\max(0, m^+ - \\|v\\|)^2 + \\lambda (1 - T) \\cdot \\max(0, \\|v\\| - m^-)^2$$\n\nwhere $T$ is the target value, $v$ is the output vector of the capsule network, $m^+$ and $m^-$ are the margins for the positive and negative classes, respectively, and $\\lambda$ is a regularization parameter that balances the loss between the two classes.\n\nAfter defining the margin loss function, we compile the model using model.compile() and set the loss to be the margin loss function.\n\nNow, when you call CapsuleNetwork(), it will return a model with the margin loss function as the optimization objective.","metadata":{}},{"cell_type":"code","source":"## Taken from https://github.com/keras-team/keras-contrib/tree/master/keras_contrib\n\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import\n\nfrom keras import backend as K\nfrom keras import activations\nfrom keras import regularizers\nfrom keras import initializers\nfrom keras import constraints\nfrom keras.layers import Layer\n\ndef to_tuple(shape):\n    \"\"\"This functions is here to fix an inconsistency between keras and tf.keras.\n\n    In tf.keras, the input_shape argument is an tuple with `Dimensions` objects.\n    In keras, the input_shape is a simple tuple of ints or `None`.\n\n    We'll work with tuples of ints or `None` to be consistent\n    with keras-team/keras. So we must apply this function to\n    all input_shapes of the build methods in custom layers.\n    \"\"\"\n    return tuple(tf.TensorShape(shape).as_list())\n\nclass Capsule(Layer):\n    \"\"\"Capsule Layer implementation in Keras\n\n       This implementation is based on Dynamic Routing of Capsules,\n       Geoffrey Hinton et. al.\n\n       The Capsule Layer is a Neural Network Layer which helps\n       modeling relationships in image and sequential data better\n       than just CNNs or RNNs. It achieves this by understanding\n       the spatial relationships between objects (in images)\n       or words (in text) by encoding additional information\n       about the image or text, such as angle of rotation,\n       thickness and brightness, relative proportions etc.\n       This layer can be used instead of pooling layers to\n       lower dimensions and still capture important information\n       about the relationships and structures within the data.\n       A normal pooling layer would lose a lot of\n       this information.\n\n       This layer can be used on the output of any layer\n       which has a 3-D output (including batch_size). For example,\n       in image classification, it can be used on the output of a\n       Conv2D layer for Computer Vision applications. Also,\n       it can be used on the output of a GRU or LSTM Layer\n       (Bidirectional or Unidirectional) for NLP applications.\n\n       The default activation function is 'linear'. But, this layer\n       is generally used with the 'squash' activation function\n       (recommended). To use the squash activation function, do :\n\n       from keras_contrib.activations import squash\n\n       capsule = Capsule(num_capsule=10,\n                         dim_capsule=10,\n                         routings=3,\n                         share_weights=True,\n                         activation=squash)\n\n       # Example usage :\n           1). COMPUTER VISION\n\n           input_image = Input(shape=(None, None, 3))\n\n           conv_2d = Conv2D(64,\n                            (3, 3),\n                            activation='relu')(input_image)\n\n           capsule = Capsule(num_capsule=10,\n                             dim_capsule=16,\n                             routings=3,\n                             activation='relu',\n                             share_weights=True)(conv_2d)\n\n           2). NLP\n\n           maxlen = 72\n           max_features = 120000\n           input_text = Input(shape=(maxlen,))\n\n           embedding = Embedding(max_features,\n                                 embed_size,\n                                 weights=[embedding_matrix],\n                                 trainable=False)(input_text)\n\n           bi_gru = Bidirectional(GRU(64,\n                                      return_seqeunces=True))(embedding)\n\n           capsule = Capsule(num_capsule=5,\n                             dim_capsule=5,\n                             routings=4,\n                             activation='sigmoid',\n                             share_weights=True)(bi_gru)\n\n       # Arguments\n           num_capsule : Number of Capsules (int)\n           dim_capsules : Dimensions of the vector output of each Capsule (int)\n           routings : Number of dynamic routings in the Capsule Layer (int)\n           share_weights : Whether to share weights between Capsules or not\n           (boolean)\n           activation : Activation function for the Capsules\n           regularizer : Regularizer for the weights of the Capsules\n           initializer : Initializer for the weights of the Caspules\n           constraint : Constraint for the weights of the Capsules\n\n       # Input shape\n            3D tensor with shape:\n            (batch_size, input_num_capsule, input_dim_capsule)\n            [any 3-D Tensor with the first dimension as batch_size]\n\n       # Output shape\n            3D tensor with shape:\n            (batch_size, num_capsule, dim_capsule)\n\n       # References\n        - [Dynamic-Routing-Between-Capsules]\n          (https://arxiv.org/pdf/1710.09829.pdf)\n        - [Keras-Examples-CIFAR10-CNN-Capsule]\"\"\"\n\n    def __init__(self,\n                 num_capsule,\n                 dim_capsule,\n                 routings=3,\n                 share_weights=True,\n                 initializer='glorot_uniform',\n                 activation=None,\n                 regularizer=None,\n                 constraint=None,\n                 **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.share_weights = share_weights\n\n        self.activation = activations.get(activation)\n        self.regularizer = regularizers.get(regularizer)\n        self.initializer = initializers.get(initializer)\n        self.constraint = constraints.get(constraint)\n\n    def build(self, input_shape):\n        input_shape = to_tuple(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1,\n                                            input_dim_capsule,\n                                            self.num_capsule *\n                                            self.dim_capsule),\n                                     initializer=self.initializer,\n                                     regularizer=self.regularizer,\n                                     constraint=self.constraint,\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule *\n                                            self.dim_capsule),\n                                     initializer=self.initializer,\n                                     regularizer=self.regularizer,\n                                     constraint=self.constraint,\n                                     trainable=True)\n\n        self.build = True\n\n    def call(self, inputs):\n        if self.share_weights:\n            u_hat_vectors = K.conv1d(inputs, self.W)\n        else:\n            u_hat_vectors = K.local_conv1d(inputs, self.W, [1], [1])\n\n        # u_hat_vectors : The spatially transformed input vectors (with local_conv_1d)\n\n        batch_size = K.shape(inputs)[0]\n        input_num_capsule = K.shape(inputs)[1]\n        u_hat_vectors = K.reshape(u_hat_vectors, (batch_size,\n                                                  input_num_capsule,\n                                                  self.num_capsule,\n                                                  self.dim_capsule))\n\n        u_hat_vectors = K.permute_dimensions(u_hat_vectors, (0, 2, 1, 3))\n        routing_weights = K.zeros_like(u_hat_vectors[:, :, :, 0])\n\n        for i in range(self.routings):\n            capsule_weights = K.softmax(routing_weights, 1)\n            outputs = K.batch_dot(capsule_weights, u_hat_vectors, [2, 2])\n            if K.ndim(outputs) == 4:\n                outputs = K.sum(outputs, axis=1)\n            if i < self.routings - 1:\n                outputs = K.l2_normalize(outputs, -1)\n                routing_weights = K.batch_dot(outputs, u_hat_vectors, [2, 3])\n                if K.ndim(routing_weights) == 4:\n                    routing_weights = K.sum(routing_weights, axis=1)\n\n        return self.activation(outputs)\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)\n\n    def get_config(self):\n        config = {'num_capsule': self.num_capsule,\n                  'dim_capsule': self.dim_capsule,\n                  'routings': self.routings,\n                  'share_weights': self.share_weights,\n                  'activation': activations.serialize(self.activation),\n                  'regularizer': regularizers.serialize(self.regularizer),\n                  'initializer': initializers.serialize(self.initializer),\n                  'constraint': constraints.serialize(self.constraint)}\n\n        base_config = super(Capsule, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import backend as K\n\n# Margin Loss function\ndef margin_loss(y_true, y_pred):\n    \"\"\"\n    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n    :param y_true: [None, n_classes]\n    :param y_pred: [None, num_capsule]\n    :return: a scalar loss value.\n    \"\"\"\n    m_plus = 0.9  # Margin for positive examples\n    m_minus = 0.1  # Margin for negative examples\n    lambda_ = 0.5  # Down-weighting parameter for the absent class\n    # Cast y_true to float32\n    y_true = K.cast(y_true, dtype='float32')\n    \n    L = y_true * K.square(K.maximum(0.0, m_plus - y_pred)) + \\\n        lambda_ * (1. - y_true) * K.square(K.maximum(0.0, y_pred - m_minus))\n\n    return K.mean(K.sum(L, axis=1))\n\n# Define the Capsule Network model\ndef CapsuleNetwork(in_features=55):\n    input_shape = (in_features,)  # Update with the number of features in your dataset\n\n    # Encoder\n    inputs = keras.Input(shape=input_shape)\n    x = layers.Reshape(target_shape=(in_features, 1))(inputs)\n    x = layers.Conv1D(filters=128, kernel_size=4, activation='relu', padding='same')(x)\n    x = layers.Conv1D(filters=128, kernel_size=8, activation='relu', strides=4, padding='same')(x)\n    x = layers.Conv1D(filters=256, kernel_size=8, activation='relu', padding='same')(x)\n    x = layers.Conv1D(filters=256, kernel_size=16, activation='relu', strides=8, padding='same')(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Reshape(target_shape=(-1, 256))(x)\n    primary_caps = Capsule(num_capsule=8, dim_capsule=16, routings=3)(x)\n\n    # DigitCaps layer\n    digit_caps = Capsule(num_capsule=1, dim_capsule=16, routings=3)(primary_caps)\n\n    # Decoder\n    x = layers.Flatten()(digit_caps)\n    x = layers.Dense(256, activation='relu')(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.1)(x)\n    outputs = layers.Dense(1, activation='sigmoid')(x)\n\n    # Create the model\n    model = keras.Model(inputs=inputs, outputs=outputs)\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The margin loss function in a capsule network combines concepts from calculus and linear algebra to optimize the network's performance and enhance its discriminative capabilities. Let's dive into the key mathematical concepts involved:\n\n1. **Vector Operations:** In linear algebra, vectors are fundamental entities that represent quantities with both magnitude and direction. In the context of a capsule network, the feature vectors can be thought of as high-dimensional vectors that encode information about the input data. These vectors can be manipulated using various vector operations, such as addition, subtraction, and dot products.\n\n2. **Sigmoid Activation:** The margin loss function operates on the outputs of the network, which are usually passed through a sigmoid activation function. The sigmoid function maps the inputs (e.g., the output logits) to a range between 0 and 1, providing a smooth and differentiable activation. This activation is crucial for applying gradient-based optimization methods, such as backpropagation, to update the network parameters.\n\n3. **Squares and Maximum Operations:** Within the margin loss function, we encounter the square and maximum operations. Squaring a value is a common operation used in optimization objectives, such as mean squared error. It ensures that all terms contribute positively to the loss, penalizing larger errors more heavily. The maximum operation, denoted as K.maximum(a, b), returns the element-wise maximum between two vectors a and b. It selects the larger value at each element position, allowing us to introduce margins into the loss calculation. For example, in the margin loss, we use K.maximum(0.0, m_plus - y_pred) to ensure that the difference between m_plus (the positive margin) and y_pred (the predicted activation) is non-negative.\n\n4. **Weighting Positive and Negative Examples:** The margin loss function accounts for the presence of positive and negative examples using class weights. These class weights are incorporated in the loss function to handle imbalanced datasets, assigning different weights to each class. The weights influence the magnitude of the loss for each example during training, allowing the network to focus more on challenging or underrepresented classes.\n\nBy leveraging these mathematical concepts, the margin loss function operates on large feature vectors to optimize the capsule network's parameters. The loss is computed by comparing the true labels (y_true) with the predicted activations (y_pred) and applying the margin constraints. The squared differences between the margins and the activations are computed using vector operations, and the final loss is determined by summing and averaging these squared differences.\n\nThe power of the margin loss function lies in its ability to optimize the network's parameters based on the margin distances between the predicted activations and the desired margins. This encourages the network to learn distinct and separable representations, enhancing its discriminative capabilities.\n\nIn summary, the margin loss function leverages concepts from calculus and linear algebra to optimize the performance of a capsule network. Through various mathematical operations on large feature vectors, it encourages the network to learn meaningful representations and adapt to imbalanced datasets.","metadata":{}},{"cell_type":"code","source":"# A utility method to create a tf.data dataset from a Pandas Dataframe\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\n    dataframe = dataframe.copy()\n    labels_in_df = False\n    if 'Class' in dataframe.columns:\n        labels_in_df = True\n        labels = dataframe.pop('Class')\n    float_columns = dataframe.select_dtypes(include=['float']).columns\n    dataframe[float_columns] = dataframe[float_columns].astype('float32')\n        \n    # Convert non-numeric values to NaN\n    dataframe[float_columns] = dataframe[float_columns].apply(pd.to_numeric, errors='coerce')\n\n    # Normalize the numerical columns\n    #scaler = StandardScaler()\n    #dataframe[float_columns] = scaler.fit_transform(dataframe[float_columns])\n        \n    # Replace missing values with 0\n    dataframe[float_columns] = dataframe[float_columns].fillna(0)\n\n    if labels_in_df:\n        ds = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(dataframe[float_columns].values, dtype=tf.float32), tf.convert_to_tensor(labels.values, dtype=tf.float32)))\n    else:\n        ds = tf.data.Dataset.from_tensor_slices(tf.convert_to_tensor(dataframe[float_columns].values, dtype=tf.float32))\n        \n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    return ds\n\ndf_to_dataset(dataset_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Harnessing the Power of Multiple Neural Networks\n\nIn the realm of machine learning, we explore a technique that combines the strengths of multiple neural networks to enhance prediction capability. This approach offers a range of benefits and considerations that shape our quest for improved accuracy and performance.\n\n### The Ensemble Approach\n\nOur methodology revolves around an ensemble of neural networks, each specialized in analyzing a specific set of features derived from our data. By segmenting the data into different feature vectors, we capture diverse patterns and intricacies that may elude a single model. This ensemble of networks functions as a collective, bringing forth a comprehensive understanding of the data landscape.\n\n### Unifying Predictions\n\nAt the core of our approach lies a meta model, which acts as a unifying entity to leverage the insights from individual networks. It integrates their predictions, creating a unified prediction that amalgamates the diverse perspectives of the constituent models. This meta model uncovers complex relationships and synergies among the features, leading to enhanced prediction capability.\n\n### Benefits and Considerations\n\nWhile this approach shows great promise, it also comes with certain considerations. Coordinating multiple neural networks and the meta model demands computational resources and careful calibration. Training and fine-tuning these models require attention to detail and optimization techniques to achieve optimal performance.\n\nInterpreting the individual neural networks' predictions may require diligent analysis and exploration. Extracting meaningful insights from their combined outputs can provide a deeper understanding of the data and improve decision-making processes.\n\n### Amplifying Prediction Capability\n\nThrough the combined efforts of multiple neural networks and the meta model, our approach offers improved prediction capability for new data. The ensemble harnesses the collective intelligence and adaptability of the models, making predictions more robust and resilient to various data scenarios. By leveraging the diverse perspectives of the individual networks, we achieve a comprehensive understanding that transcends the limitations of a single model.\n\nIn our pursuit of accuracy and performance, this methodology empowers us to harness the power of multiple neural networks. With careful calibration, analysis, and interpretation, we unlock the potential for superior predictions, advancing our understanding and decision-making in the domain of machine learning.\n","metadata":{}},{"cell_type":"code","source":"from sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier, export_text\nfrom sklearn import tree\nfrom pandas.api.types import is_numeric_dtype\nfrom sklearn.tree import export_graphviz \nimport cv2\n  \ndef convert_cats(df):\n        cats = []\n        for col in df.columns:\n            if is_numeric_dtype(df[col]):\n                pass\n            else:\n                cats.append(col)\n        for col in cats:\n            df[col] = df[col].astype('category')\n            \n        return df\n\nfull_df = dataset_df.copy()\n\nfloat_columns = full_df.select_dtypes(include=['float']).columns\nfull_df[float_columns] = full_df[float_columns].astype('float32')\nfull_df = convert_cats(full_df)\n# Convert non-numeric values to NaN\nfull_df[float_columns] = full_df[float_columns].apply(pd.to_numeric, errors='coerce')\n\n# Replace missing values with 0\nfull_df[float_columns] = full_df[float_columns].fillna(0)\n\n# Fit the regressor, set max_depth = 3\ndecision_tree = DecisionTreeClassifier(max_leaf_nodes=8, class_weight=class_weight, random_state=int(time.time()))\ndecision_tree = decision_tree.fit(full_df[float_columns], full_df['Class'])\n\nplt.figure(figsize=(24,16))\ntree.plot_tree(decision_tree, feature_names=float_columns, class_names='Class')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the decision paths\ndecision_paths = decision_tree.decision_path(full_df[float_columns]).toarray()\n\n# Get the column names\ncolumn_names = decision_tree.feature_names_in_\n\n# Create a list of feature lists for each path\nfeature_lists = []\nfor path in decision_paths:\n    feature_list = []\n    for node, value in enumerate(path):\n        if value == 1:\n            feature_list.append(column_names[decision_tree.tree_.feature[node]])\n    feature_lists.append(feature_list)\n    \ntop_feature_lists = list(set(tuple(feature_list) for feature_list in feature_lists))\ntop_feature_lists += [set(float_columns)-set([item for sublist in top_feature_lists for item in sublist])]\ntop_feature_lists = [sorted(list(f)) for f in top_feature_lists]\ntop_feature_lists","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_model(corresponding_df):\n    feat_list = corresponding_df.columns\n    # Define the feature columns list\n    feature_columns = []\n\n    # Iterate over the columns in the dataset\n    for column in feat_list:\n        if column != 'Class':  # Exclude the target column if present\n            if dataset_df[column].dtype == 'object':\n                # Categorical column\n                vocab = dataset_df[column].unique()\n                cat_column = tf.feature_column.categorical_column_with_vocabulary_list(column, vocab)\n                feature_columns.append(tf.feature_column.indicator_column(cat_column))\n            else:\n                # Numeric column\n                feature_columns.append(tf.feature_column.numeric_column(column))\n    \n#     # Define the input layer\n#     input_layer = tf.keras.layers.Input(shape=(len(feature_columns),))\n    \n#     # Reshape the input layer to match the expected input shape for the CNN\n#     reshaped_input = tf.keras.layers.Reshape((-1, 1))(input_layer)\n\n#     # Convolutional layers\n#     conv1 = tf.keras.layers.Conv1D(32, 3, activation='relu', padding='same')(reshaped_input)\n#     conv2 = tf.keras.layers.Conv1D(64, 3, activation='relu', padding='same')(conv1)\n#     conv3 = tf.keras.layers.Conv1D(128, 3, activation='relu', padding='same')(conv2)\n\n#     # Flatten the convolutional outputs\n#     flattened = tf.keras.layers.Flatten()(conv3)\n  \n#     # Define the hidden layers\n#     x = layers.Dense(128, activation='relu')(flattened)\n#     x = layers.Dense(128, activation='relu')(x)\n#     x = layers.Flatten()(x)\n#     x = layers.Dropout(0.1)(x)\n\n#     # Define the output layer\n#     outputs = layers.Dense(1)(x)\n    \n#     # Create the model\n#     model = tf.keras.Model(inputs=input_layer, outputs=outputs)\n    model = CapsuleNetwork(in_features=len(feature_columns))\n    return model\n\nmodels = [create_model(dataset_df[list(flist)]) for flist in top_feature_lists]\n\n# Combine the outputs of individual models\noutputs = [model.output for model in models]  # Access the second-to-last layer\n\n# Define the meta_model architecture\nmeta_model_input = layers.concatenate(outputs)\nmeta_model_hidden = layers.Dense(36, activation='relu')(meta_model_input)\nmeta_model_output = layers.Dense(1, activation='sigmoid')(meta_model_hidden)\n\n# Create the meta_model\nmeta_model = tf.keras.models.Model(inputs=[model.input for model in models], outputs=meta_model_output)\n\n# Compile the meta_model\nmeta_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\n# Assuming you have already created the meta_model\n# ...\n\n# Visualize the meta_model\nplot_model(meta_model, to_file='meta_model.png', show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\n\n# Initialize an empty list to store the predictions of individual models:\nmodel_predictions = []\nmeta_models = {}\nfor model in models:\n    model.compile(optimizer='adam',\n              #loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n                loss=margin_loss,\n              metrics=['accuracy'])\n\nepoch_i = 1\n# Iterate through each fold of the k-fold cross-validation:\nfor i, (train_index, valid_index) in enumerate(kf.split(X=dataset_df, y=dataset_df['Class'])):\n    print('##### Fold', i+1)\n    \n    # Fetch values corresponding to the index\n    train_df = dataset_df.iloc[train_index]\n    valid_df = dataset_df.iloc[valid_index]\n    valid_ids = valid_df.index.values\n\n    # Train the individual models on train_data and make predictions on valid_data\n    model_predictions_fold = []\n    for model, feat_list in zip(models,top_feature_lists):\n        # Define the ReduceLROnPlateau and EarlyStopping callback\n        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n        early_stopping = EarlyStopping(monitor='val_loss', patience=8, verbose=1)\n        # Prepare the train_data and valid_data using df_to_dataset\n        train_data = df_to_dataset(train_df[feat_list+['Class']])\n        valid_data = df_to_dataset(valid_df[feat_list+['Class']])\n\n        model.fit(train_data, epochs=50, batch_size=BATCH_SIZE,validation_data=valid_data, class_weight=class_weight, \n              callbacks=[reduce_lr, early_stopping ])\n        predictions = model.predict(train_data)\n        model_predictions_fold.append(predictions)\n\n    # Store the predictions of individual models for this fold\n    model_predictions.append(model_predictions_fold)\n    # Define the ReduceLROnPlateau and EarlyStopping callback\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n    early_stopping = EarlyStopping(monitor='val_loss', patience=15, verbose=1)\n\n    datasets = [tfds.as_numpy(df_to_dataset(train_df[feat_list]).rebatch(len(train_df))) for feat_list in top_feature_lists]\n    valid_datasets = [tfds.as_numpy(df_to_dataset(valid_df[feat_list]).rebatch(len(valid_df))) for feat_list in top_feature_lists]\n    meta_model.fit([y for x in datasets for y in x],\n               tf.convert_to_tensor(train_df['Class'].values), \n               epochs=200, \n               validation_data=([y for x in valid_datasets for y in x],valid_df['Class'].values),\n                    batch_size=BATCH_SIZE, callbacks=[reduce_lr, early_stopping ])\n    \n\n    # Store the model\n    meta_models[f\"fold_{i+1}\"] = meta_model\n    #epoch_i += len(meta_model.history.history['loss']) - 1\n\n    # Plot loss\n    plt.figure(figsize=(14,14))\n    plt.plot(meta_model.history.history['loss'])\n    plt.plot(meta_model.history.history['val_loss'])\n    plt.title(f'Model Loss - Fold {i+1}')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper right')\n    plt.show()\n\n    # Plot test accuracy\n    plt.figure(figsize=(14,14))\n    plt.plot(meta_model.history.history['accuracy'])\n    plt.plot(meta_model.history.history['val_accuracy'])\n    plt.title(f'Model Accuracy - Fold {i+1}')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='lower right')\n    plt.show()\n\n    # Predict on the validation data\n    predictions = meta_model.predict([y for x in valid_datasets for y in x])\n\n    # Store the predictions in the oof dataframe\n    oof.loc[valid_ids, 0] = predictions.flatten()   \n\n    # Evaluate and store the metrics in respective dicts\n    evaluation = meta_model.evaluate([y for x in valid_datasets for y in x],valid_df['Class'].values,return_dict=True, use_multiprocessing=True)\n    accuracy[f\"fold_{i+1}\"] = evaluation[\"accuracy\"]\n    margin_loss_dict[f\"fold_{i+1}\"]= evaluation[\"loss\"]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#datasets\ntrain_df[feat_list+['Class']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BATCH_SIZE = 8\n# # Define the Capsule Network model\n# #model = CapsuleNetwork()\n\n# # Compile the model\n# #model.compile(optimizer='adam', loss=margin_loss, metrics=['accuracy'])\n\n# model.compile(optimizer='adam',\n#               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n#               metrics=['accuracy'])\n\n# # Define the ReduceLROnPlateau and EarlyStopping callback\n# reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n# early_stopping = EarlyStopping(monitor='val_loss', patience=8, verbose=1)\n\n# epoch_i = 1\n# # Loop through each fold\n# for i, (train_index, valid_index) in enumerate(kf.split(X=dataset_df,y=dataset_df['Class'])):\n#     print('##### Fold',i+1)\n\n#     # Fetch values corresponding to the index \n#     train_df = dataset_df.iloc[train_index]\n#     valid_df = dataset_df.iloc[valid_index]\n#     valid_ids = valid_df.index.values\n\n#     train_data = df_to_dataset(train_df)\n#     valid_data = df_to_dataset(valid_df)\n\n#     # Train the model\n#     #model.fit(train_data, train_target, epochs=20, batch_size=BATCH_SIZE, validation_data=(valid_data,valid_target), class_weight=class_weight, use_multiprocessing=True, callbacks=[reduce_lr, early_stopping ])\n#     model.fit(train_data, epochs=1000, batch_size=BATCH_SIZE, validation_data=valid_data, class_weight=class_weight, \n#               use_multiprocessing=True, callbacks=[reduce_lr, early_stopping ], initial_epoch=epoch_i)\n#     # Store the model\n#     models[f\"fold_{i+1}\"] = model\n#     epoch_i += len(model.history.history['loss']) - 1\n\n#     # Plot loss\n#     plt.figure(figsize=(14,14))\n#     plt.plot(model.history.history['loss'])\n#     plt.plot(model.history.history['val_loss'])\n#     plt.title(f'Model Loss - Fold {i+1}')\n#     plt.ylabel('Loss')\n#     plt.xlabel('Epoch')\n#     plt.legend(['Train', 'Validation'], loc='upper right')\n#     plt.show()\n\n#     # Plot test accuracy\n#     plt.figure(figsize=(14,14))\n#     plt.plot(model.history.history['accuracy'])\n#     plt.plot(model.history.history['val_accuracy'])\n#     plt.title(f'Model Accuracy - Fold {i+1}')\n#     plt.ylabel('Accuracy')\n#     plt.xlabel('Epoch')\n#     plt.legend(['Train', 'Validation'], loc='lower right')\n#     plt.show()\n\n#     # Predict on the validation data\n#     predictions = model.predict(valid_data)\n\n#     # Store the predictions in the oof dataframe\n#     oof.loc[valid_ids, 0] = predictions.flatten()   \n\n#     # Evaluate and store the metrics in respective dicts\n#     evaluation = model.evaluate(valid_data,return_dict=True, use_multiprocessing=True)\n#     accuracy[f\"fold_{i+1}\"] = evaluation[\"accuracy\"]\n#     margin_loss_dict[f\"fold_{i+1}\"]= evaluation[\"loss\"]","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The capsule network architecture exhibits remarkable performance in terms of loss functions and accuracy, both during training and validation. The effectiveness of this network can be attributed to its ingenious design, incorporating principles that enable it to excel in handling complex data.\n\nOne key feature of the capsule network is its hierarchical representation. Unlike traditional convolutional neural networks (CNNs) that rely heavily on pooling layers, which may inadvertently discard vital spatial information, capsule networks leverage capsules - groups of neurons representing specific entities or parts. By encoding the presence and properties of these entities, capsules provide a more nuanced and structured representation of the data.\n\nA crucial component of the capsule network is the dynamic routing algorithm, which facilitates effective communication between capsules. Through dynamic routing, information is accurately routed and aggregated, ensuring that the outputs of capsules are updated based on consensus between lower-level and higher-level capsules. This dynamic routing mechanism refines the network's representation, resulting in improved overall performance.\n\nThe capsule network's distinct feature is the separation of pose and activation vectors. Unlike conventional networks that primarily rely on activation values, capsule networks additionally utilize pose vectors, encoding properties and transformation parameters of the entities. By optimizing both activation and pose vectors, the network can more effectively capture intricate patterns and discern between different classes.\n\nMoreover, the inclusion of a reconstruction loss in the capsule network further enhances its capabilities. The decoder network in the network's architecture reconstructs the input vector from the pose vector of the predicted class. This reconstruction loss function encourages the network to learn meaningful representations while effectively regularizing its performance. By reconstructing the input, the network can capture essential features while reducing the influence of noise or extraneous variations in the data.\n\nThese elements, in combination, contribute to the impressive performance exhibited by the capsule network on your data. The hierarchical representation, dynamic routing, and the separation of pose and activation vectors enable the network to adeptly capture complex patterns and relationships. Furthermore, the incorporation of the reconstruction loss encourages the network to learn meaningful representations, leading to improved discrimination capabilities. In essence, the capsule network's unique architecture and design choices make it exceptionally well-suited for handling complex data, resulting in exemplary performance metrics such as low loss and high accuracy during both training and validation stages.","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport IPython.display as display\nfrom tensorflow.keras.utils import plot_model\n\n# Plot the model architecture\nplot_model(meta_models['fold_1'], to_file='model.png', show_shapes=True, show_layer_names=True)\n\n# Display the model architecture diagram\ndisplay.Image(\"model.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, let us check the evaluation metrics for each fold and its average value.","metadata":{}},{"cell_type":"code","source":"average_loss = 0\naverage_acc = 0\nbest_model = None\nbest_accuracy = 0.0\nbest_loss = float('inf')\n\nfor _model in meta_models:\n    average_loss += margin_loss_dict[_model]\n    average_acc += accuracy[_model]\n    print(f\"{_model}: acc: {accuracy[_model]:.4f} loss: {margin_loss_dict[_model]:.4f}\")\n    # Update the best model if the current model has higher accuracy and lower loss\n    if accuracy[_model] >= best_accuracy or (accuracy[_model] == best_accuracy and margin_loss_dict[_model] <= best_loss):\n        best_accuracy = accuracy[_model]\n        best_loss = margin_loss_dict[_model]\n        best_model = meta_models[_model]\n\n\nprint(f\"\\nAverage accuracy: {average_acc/10:.4f}  Average loss: {average_loss/10:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Variable importances\n\nVariable importances generally indicate how much a feature contributes to the model predictions or quality. There are several ways to identify important features using TensorFlow Neural Networks. Let us pick one model from models dict and inspect it.\n\n## Permutation Importance\n\nThis approach evaluates the impact of shuffling the values of each feature on the model's performance. By measuring the drop in performance after permuting a feature, you can estimate its importance.","metadata":{}},{"cell_type":"markdown","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.base import BaseEstimator \nfrom sklearn.base import RegressorMixin\nvalid_datasets = [tfds.as_numpy(df_to_dataset(valid_df[feat_list]).rebatch(len(valid_df))) for feat_list in top_feature_lists]\nclass ModelWrapper(BaseEstimator, RegressorMixin):\n    def __init__(self, model):\n        self.model = model\n\n    def fit(self, X, y):\n        return self.model.fit(X,y)\n\n    def predict(self, X):\n        return self.model.predict(X)\n\n# Create a model wrapper instance\nmodel_wrapper = ModelWrapper(best_model)\n\n# Define a scoring function based on model predictions\ndef scoring_function(model_wrapper, X, y):\n    y_pred = model_wrapper.predict(X)\n    # Convert continuous values to binary labels\n    valid_target_binary = (y_pred > 0.5).astype(int)\n    return accuracy_score(y, valid_target_binary)\n\n# Compute permutation importances\nresult = permutation_importance(model_wrapper, [tf.convert_to_tensor(y) for x in valid_datasets for y in x], valid_df['Class'].values, scoring=scoring_function, n_repeats=10, random_state=42)\n\n# Get feature importances from the result\nimportances = result.importances_mean\n\n# Visualize the feature importances\nplt.bar(range(len(importances)), importances)\nplt.xlabel('Feature Index')\nplt.ylabel('Importance')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-07-17T15:52:40.511816Z","iopub.execute_input":"2023-07-17T15:52:40.512447Z","iopub.status.idle":"2023-07-17T15:52:40.909811Z","shell.execute_reply.started":"2023-07-17T15:52:40.512386Z","shell.execute_reply":"2023-07-17T15:52:40.906917Z"}}},{"cell_type":"markdown","source":"m = [tf.convert_to_tensor(y) for x in valid_datasets for y in x]\nm","metadata":{"execution":{"iopub.status.busy":"2023-07-17T15:52:05.976668Z","iopub.execute_input":"2023-07-17T15:52:05.977532Z","iopub.status.idle":"2023-07-17T15:52:06.018632Z","shell.execute_reply.started":"2023-07-17T15:52:05.977488Z","shell.execute_reply":"2023-07-17T15:52:06.016509Z"}}},{"cell_type":"markdown","source":"In this code, model represents our trained TensorFlow neural network model. The permutation_importance() function from scikit-learn is used to compute the permutation importances. The scoring_function() is a custom scoring function that computes the performance metric (e.g., accuracy) based on the model predictions. Finally, a bar plot is created to visualize the feature importances.\n\nNote that the above approaches are general techniques and may require modifications based on the specifics of your model and problem. Additionally, there are other techniques available for computing variable importances, such as SHAP values and LIME. The choice of method depends on the nature of the model and the insights we seek.\n\n### Other Permutation Importance Visualizations\nWe have already created a bar plot to visualize the feature importances. We can enhance this plot by sorting the features in descending order of importance and adding error bars to represent the variability of the importance scores across repetitions.","metadata":{}},{"cell_type":"markdown","source":"# Sort features by importance\nsorted_indices = np.argsort(importances)[::-1]\nsorted_importances = importances[sorted_indices]\n\n# Plot feature importances\nplt.figure(figsize=(16,16))\nplt.bar(range(len(sorted_importances)), sorted_importances)\nplt.errorbar(range(len(sorted_importances)), sorted_importances, yerr=result.importances_std[sorted_indices], fmt='.k')\nplt.xticks(range(len(sorted_importances)), sorted_indices)\nplt.xlabel('Feature Index')\nplt.ylabel('Importance')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-17T15:29:55.114068Z","iopub.status.idle":"2023-07-17T15:29:55.117603Z","shell.execute_reply.started":"2023-07-17T15:29:55.117297Z","shell.execute_reply":"2023-07-17T15:29:55.117327Z"}}},{"cell_type":"markdown","source":"#### Feature Importance Curve\n\nNext, wee create a cumulative feature importance curve to show the cumulative importance of features. This plot helps identify the most important features and their cumulative contribution.","metadata":{}},{"cell_type":"markdown","source":"# Calculate cumulative importance\ncumulative_importance = np.cumsum(sorted_importances)\n\n# Plot cumulative importance curve\nplt.figure(figsize=(11,11))\nplt.plot(range(1, len(cumulative_importance) + 1), cumulative_importance)\nplt.xlabel('Number of Features')\nplt.ylabel('Cumulative Importance')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-17T15:29:55.119288Z","iopub.status.idle":"2023-07-17T15:29:55.120278Z","shell.execute_reply.started":"2023-07-17T15:29:55.120041Z","shell.execute_reply":"2023-07-17T15:29:55.120065Z"}}},{"cell_type":"markdown","source":"#### Heatmap\n\nAs we have a large number of features,we can create a heatmap to visualize the permutation importances across features. This can help identify patterns and relationships between features.","metadata":{}},{"cell_type":"markdown","source":"# Create a heatmap of permutation importances\nimport seaborn as sns\n\n# Reshape the importances array to match the data shape\nimportance_matrix = importances.reshape((1, -1))\n\n# Plot the heatmap\nplt.figure(figsize=(12,12))\nsns.heatmap(importance_matrix, cmap='viridis', annot=True, cbar=True)\nplt.xlabel('Feature Index')\nplt.ylabel('Importance')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-07-17T15:29:55.121207Z","iopub.status.idle":"2023-07-17T15:29:55.121584Z","shell.execute_reply.started":"2023-07-17T15:29:55.121392Z","shell.execute_reply":"2023-07-17T15:29:55.121409Z"}}},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv').set_index(\"Id\")\nprint(test_df)\ntest_df.columns = test_df.columns.str.strip()\ntest_datasets = [tfds.as_numpy(df_to_dataset(test_df[feat_list]).rebatch(len(test_df))) for feat_list in top_feature_lists]\ntest_ds = [y for x in test_datasets for y in x]\n#test_ds = df_to_dataset(test_df, shuffle=False)\npredictions = best_model.predict(test_ds, batch_size=1)\nn_predictions= [[round(abs(i-1), 8), i] for i in predictions.ravel()]\nprint(n_predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df\n#n_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/icr-identify-age-related-conditions/sample_submission.csv\")\nsample_submission[['class_0', 'class_1']] = n_predictions\nsample_submission.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}