{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CAFA 5 protein function Prediction with TensorFlow\n\nThis notebook walks you through how to train a DNN model using TensorFlow on the CAFA 5 protein function Prediction dataset made available for this competition.\n\nThe objective of the model is to predict the function(aka **GO term ID**) of a set of proteins based on their amino acid sequences and other data.\n\n\n**Note** : This notebook runs without any GPU. This is because enabling GPUs leaves less RAM memory on the VM and the submission step needs a lot of memory. One point where this would impact is when training the model. With CPU it will take around 2 minutes while on GPU it would take around 30 seconds.","metadata":{"papermill":{"duration":0.008875,"end_time":"2023-05-09T08:30:09.052999","exception":false,"start_time":"2023-05-09T08:30:09.044124","status":"completed"},"tags":[],"id":"pb-4zvGq1K4C"}},{"cell_type":"markdown","source":"## About the Data\n\n### Protein Sequence\n\nEach protein is composed of dozens or hundreds of amino acids that are linked sequentially. Each amino acid in the sequence may be represented by a one-letter or three-letter code. Thus the sequence of a protein is often notated as a string of letters.\n\n<img src=\"https://cityu-bioinformatics.netlify.app/img/tools/protein/pro_seq.png\" alt =\"Sequence.png\" style='width: 800px;' >\n\nImage source - [https://cityu-bioinformatics.netlify.app/](https://cityu-bioinformatics.netlify.app/too2/new_proteo/pro_seq/)\n\nThe `train_sequences.fasta` made available for this competitions, contains the sequences for proteins with annotations (labelled proteins).","metadata":{"papermill":{"duration":0.009266,"end_time":"2023-05-09T08:30:09.071132","exception":false,"start_time":"2023-05-09T08:30:09.061866","status":"completed"},"tags":[],"id":"B8H6QR-M1K4J"}},{"cell_type":"markdown","source":"# Gene Ontology\n\nWe can define the functional properties of a proteins using Gene Ontology(GO). Gene Ontology (GO) describes our understanding of the biological domain with respect to three aspects:\n1. Molecular Function (MF)\n2. Biological Process (BP)\n3. Cellular Component (CC)\n\nRead more about Gene Ontology [here](http://geneontology.org/docs/ontology-documentation).\n\nFile `train_terms.tsv` contains the list of annotated terms (ground truth) for the proteins in `train_sequences.fasta`. In `train_terms.tsv` the first column indicates the protein's UniProt accession ID (unique protein id), the second is the `GO Term ID`, and the third indicates in which ontology the term appears.","metadata":{"papermill":{"duration":0.008355,"end_time":"2023-05-09T08:30:09.08863","exception":false,"start_time":"2023-05-09T08:30:09.080275","status":"completed"},"tags":[],"id":"vYXassg_1K4L"}},{"cell_type":"markdown","source":"# Labels of the dataset\n\nThe objective of our model is to predict the terms (functions) of a protein sequence. One protein sequence can have many functions and can thus be classified into any number of terms. Each term is uniquely identified by a `GO Term ID`. Thus our model has to predict all the `GO Term ID`s for a protein sequence. This means that the task at hand is a multi-label classification problem.","metadata":{"id":"U1wpCeJH1K4M"}},{"cell_type":"markdown","source":"\n","metadata":{"papermill":{"duration":0.008308,"end_time":"2023-05-09T08:30:09.105539","exception":false,"start_time":"2023-05-09T08:30:09.097231","status":"completed"},"tags":[],"id":"M042PgfH1K4N"}},{"cell_type":"markdown","source":"# Protein embeddings for train and test data\n\nTo train a machine learning model we cannot use the alphabetical protein sequences in`train_sequences.fasta` directly. They have to be converted into a vector format. In this notebook, we will use embeddings of the protein sequences to train the model. You can think of protein embeddings to be similar to word embeddings used to train NLP models.\n<!-- Instead, to make calculations and data preparation easier we will use precalculated protein embeddings.\n -->\nProtein embeddings are a machine-friendly method of capturing the protein's structural and functional characteristics, mainly through its sequence. One approach is to train a custom ML model to learn the protein embeddings of the protein sequences in the dataset being used in this notebook. Since this dataset represents proteins using amino-acid sequences which is a standard approach, we can use any publicly available pre-trained protein embedding models to generate the embeddings.\n\nThere are a variety of protein embedding models. To make data preparation easier, we have used the precalculated protein embeddings created by [Sergei Fironov](https://www.kaggle.com/sergeifironov) using the Rost Lab's T5 protein language model in this notebook. The precalculated protein embeddings can be found [here](https://www.kaggle.com/datasets/sergeifironov/t5embeds). We have added this dataset to the notebook along with the dataset made available for the competition.\n\nTo add this to your enviroment, on the right side panel, click on `Add Data` and search for `t5embeds` (make sure that it's the correct [one](https://www.kaggle.com/datasets/sergeifironov/t5embeds)) and then click on the `+` beside it.\n\n**We'll start by introducing a variable that decides whether we prepare the training data cell by cell or if we do it all at once (for memory issues)**\n","metadata":{"papermill":{"duration":0.008548,"end_time":"2023-05-09T08:30:09.122603","exception":false,"start_time":"2023-05-09T08:30:09.114055","status":"completed"},"tags":[],"id":"tD6exQ-11K4N"}},{"cell_type":"code","source":"notebook_run = False","metadata":{"execution":{"iopub.status.busy":"2023-07-19T03:34:36.841447Z","iopub.execute_input":"2023-07-19T03:34:36.842200Z","iopub.status.idle":"2023-07-19T03:34:36.847705Z","shell.execute_reply.started":"2023-07-19T03:34:36.842152Z","shell.execute_reply":"2023-07-19T03:34:36.846508Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Import the Required Libraries","metadata":{"papermill":{"duration":0.009086,"end_time":"2023-05-09T08:30:09.140473","exception":false,"start_time":"2023-05-09T08:30:09.131387","status":"completed"},"tags":[],"id":"HeXcmUEb1K4O"}},{"cell_type":"code","source":"!pip install networkx Bio\n!pip install torch-geometric torch-scatter","metadata":{"id":"9oq-a6oG1K4P","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc85caf6-e48e-4031-ee0a-9e74da3cfab6","execution":{"iopub.status.busy":"2023-07-19T03:34:36.870195Z","iopub.execute_input":"2023-07-19T03:34:36.870943Z","iopub.status.idle":"2023-07-19T03:46:30.504992Z","shell.execute_reply.started":"2023-07-19T03:34:36.870914Z","shell.execute_reply":"2023-07-19T03:46:30.503763Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (3.1)\nCollecting Bio\n  Downloading bio-1.5.9-py3-none-any.whl (276 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: biopython>=1.80 in /opt/conda/lib/python3.10/site-packages (from Bio) (1.81)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from Bio) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from Bio) (4.65.0)\nCollecting mygene (from Bio)\n  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from Bio) (1.5.3)\nRequirement already satisfied: pooch in /opt/conda/lib/python3.10/site-packages (from Bio) (1.6.0)\nCollecting gprofiler-official (from Bio)\n  Downloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from biopython>=1.80->Bio) (1.23.5)\nCollecting biothings-client>=0.2.6 (from mygene->Bio)\n  Downloading biothings_client-0.3.0-py2.py3-none-any.whl (29 kB)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->Bio) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->Bio) (2023.3)\nRequirement already satisfied: appdirs>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from pooch->Bio) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pooch->Bio) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->Bio) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->Bio) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->Bio) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->Bio) (2023.5.7)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pooch->Bio) (3.0.9)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->Bio) (1.16.0)\nInstalling collected packages: gprofiler-official, biothings-client, mygene, Bio\nSuccessfully installed Bio-1.5.9 biothings-client-0.3.0 gprofiler-official-1.0.0 mygene-3.2.2\nCollecting torch-geometric\n  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting torch-scatter\n  Downloading torch_scatter-2.1.1.tar.gz (107 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.6/107.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.65.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.23.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.11.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.31.0)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2023.5.7)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.1.0)\nBuilding wheels for collected packages: torch-geometric, torch-scatter\n  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=3e4891ce374f69a30eb1b023e80730f191f1ff1f178218642050eaf64576d4fb\n  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n  Building wheel for torch-scatter (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torch-scatter: filename=torch_scatter-2.1.1-cp310-cp310-linux_x86_64.whl size=3751351 sha256=77539e7852da74cb4cff2b4aaeacd0fbe1803fd5ff9bbb307b52efe665bd12e5\n  Stored in directory: /root/.cache/pip/wheels/ef/67/58/6566a3b61c6ec0f2ca0c2c324cd035ef2955601f0fb3197d5f\nSuccessfully built torch-geometric torch-scatter\nInstalling collected packages: torch-scatter, torch-geometric\nSuccessfully installed torch-geometric-2.3.1 torch-scatter-2.1.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom scipy.sparse import coo_matrix\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GINConv, global_add_pool, GCNConv, global_max_pool, global_mean_pool, GATConv, SAGEConv\nfrom torch_geometric.data import Data, Batch\nfrom torch_geometric.loader.dataloader import DataLoader\nfrom torch.utils.data import Dataset\n\nfrom torch_geometric.utils import from_networkx, to_networkx, to_dense_batch, convert\nfrom torch.nn import Sequential, Linear, BatchNorm1d, ReLU, SELU\nfrom torch_geometric.nn.models import GAT\nfrom sklearn.model_selection import train_test_split\nimport torch.optim as optim\nimport torch_geometric.nn.models as models\nfrom Bio import SeqIO\nfrom sklearn.metrics import classification_report\nimport concurrent.futures\n\n# Required for progressbar widget\nimport progressbar\nfrom datetime import datetime\nimport random\nrandom.seed(datetime.now().timestamp())","metadata":{"papermill":{"duration":9.85331,"end_time":"2023-05-09T08:30:19.002985","exception":false,"start_time":"2023-05-09T08:30:09.149675","status":"completed"},"tags":[],"id":"nb0xJgIp1K4R","execution":{"iopub.status.busy":"2023-07-19T05:40:11.400933Z","iopub.execute_input":"2023-07-19T05:40:11.401543Z","iopub.status.idle":"2023-07-19T05:40:11.411290Z","shell.execute_reply.started":"2023-07-19T05:40:11.401507Z","shell.execute_reply":"2023-07-19T05:40:11.410083Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"print(\"Numpy v\" + np.__version__)\nkaggle_input_data = '/kaggle/input/cafa-5-protein-function-prediction'","metadata":{"papermill":{"duration":0.018272,"end_time":"2023-05-09T08:30:19.030432","exception":false,"start_time":"2023-05-09T08:30:19.01216","status":"completed"},"tags":[],"id":"x6XxiN4L1K4S","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b22d5bd0-a35c-4def-80e2-484c17d35a4b","execution":{"iopub.status.busy":"2023-07-19T04:15:00.259913Z","iopub.execute_input":"2023-07-19T04:15:00.260595Z","iopub.status.idle":"2023-07-19T04:15:00.279008Z","shell.execute_reply.started":"2023-07-19T04:15:00.260559Z","shell.execute_reply":"2023-07-19T04:15:00.277470Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Numpy v1.23.5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load the Dataset","metadata":{"papermill":{"duration":0.008429,"end_time":"2023-05-09T08:30:19.047756","exception":false,"start_time":"2023-05-09T08:30:19.039327","status":"completed"},"tags":[],"id":"WU7qEirr1K4T"}},{"cell_type":"markdown","source":"First we will load the file `train_terms.tsv` which contains the list of annotated terms (functions) for the proteins. We will extract the labels aka `GO term ID` and create a label dataframe for the protein embeddings.","metadata":{"papermill":{"duration":0.008388,"end_time":"2023-05-09T08:30:19.065367","exception":false,"start_time":"2023-05-09T08:30:19.056979","status":"completed"},"tags":[],"id":"Fn7EjFw01K4T"}},{"cell_type":"code","source":"if notebook_run:\n    #train_terms = pd.read_csv(\"/kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv\",sep=\"\\t\")\n    train_terms = pd.read_csv(kaggle_input_data + \"/Train/train_terms.tsv\",sep=\"\\t\")\n    print(train_terms.shape)","metadata":{"papermill":{"duration":3.69155,"end_time":"2023-05-09T08:30:22.766144","exception":false,"start_time":"2023-05-09T08:30:19.074594","status":"completed"},"tags":[],"id":"ggKV4PCs1K4T","colab":{"base_uri":"https://localhost:8080/"},"outputId":"25f031ff-bab8-4cc5-ca0d-21e8d38de8cd","execution":{"iopub.status.busy":"2023-07-19T04:15:00.280688Z","iopub.execute_input":"2023-07-19T04:15:00.281445Z","iopub.status.idle":"2023-07-19T04:15:00.288519Z","shell.execute_reply.started":"2023-07-19T04:15:00.281413Z","shell.execute_reply":"2023-07-19T04:15:00.287462Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"`train_terms` dataframe is composed of 3 columns and 5363863 entries. We can see all 3 dimensions of our dataset by printing out the first 5 entries using the following code:","metadata":{"papermill":{"duration":0.008358,"end_time":"2023-05-09T08:30:22.783293","exception":false,"start_time":"2023-05-09T08:30:22.774935","status":"completed"},"tags":[],"id":"Vr98Z1qX1K4T"}},{"cell_type":"code","source":"if notebook_run:\n    train_terms = train_terms.sample(150000)","metadata":{"papermill":{"duration":0.038607,"end_time":"2023-05-09T08:30:22.830633","exception":false,"start_time":"2023-05-09T08:30:22.792026","status":"completed"},"tags":[],"id":"Cvb-XyY91K4U","execution":{"iopub.status.busy":"2023-07-19T04:15:00.292016Z","iopub.execute_input":"2023-07-19T04:15:00.292825Z","iopub.status.idle":"2023-07-19T04:15:00.300543Z","shell.execute_reply.started":"2023-07-19T04:15:00.292767Z","shell.execute_reply":"2023-07-19T04:15:00.299022Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"if notebook_run:\n    train_terms.loc[train_terms['EntryID'] == 'Q59VP0']","metadata":{"scrolled":true,"id":"brMGEGHw1K4U","colab":{"base_uri":"https://localhost:8080/","height":49},"outputId":"4832995c-95d7-4f41-f391-2935aaf1ee76","execution":{"iopub.status.busy":"2023-07-19T04:15:00.302207Z","iopub.execute_input":"2023-07-19T04:15:00.302709Z","iopub.status.idle":"2023-07-19T04:15:00.308857Z","shell.execute_reply.started":"2023-07-19T04:15:00.302678Z","shell.execute_reply":"2023-07-19T04:15:00.307885Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"If we look at the first entry of `train_terms.tsv`, we can see that it contains protein id(`A0A009IHW8`), the GO term(`GO:0008152`) and its aspect(`BPO`).","metadata":{"papermill":{"duration":0.008764,"end_time":"2023-05-09T08:30:22.848867","exception":false,"start_time":"2023-05-09T08:30:22.840103","status":"completed"},"tags":[],"id":"HsxDFooS1K4U"}},{"cell_type":"markdown","source":"# A different kind of embedding\n\nWe introduce a new embedding for Proteins based on the notion of Recurrence Networks: Olyaee, M. H., Yaghoubi, A., & Yaghoobi, M. (2016). Predicting protein structural classes based on complex networks and recurrence analysis. Journal of theoretical biology, 404, 375–382. https://doi.org/10.1016/j.jtbi.2016.06.018\n\nInstead of using the provided embeddings, we will train Graph Neural Networks on a new embedding for each graph, defined by recurrence networks.\n\n**Let's start by defining a function that takes a Protein Sequence String and creates a recurrence network:**","metadata":{"id":"MbqodfXE1K4W"}},{"cell_type":"markdown","source":"","metadata":{"id":"NWJhqhFmlVzV"}},{"cell_type":"code","source":"from Bio.SeqUtils.ProtParamData import *\nfrom collections import Counter\n\ndef protein_recurrence_network(sequence):\n    # Create an empty graph for the recurrence network\n    graph = nx.DiGraph()\n\n    # Use a Counter to store the edge weights\n    edge_weights = Counter(zip(sequence, sequence[1:]))\n\n    # Add nodes and edges to the graph based on the edge weights using list comprehension\n    graph.add_weighted_edges_from(((char1, char2, weight / len(sequence)) \n                                    for (char1, char2), weight in edge_weights.items()))\n    return protein_attributes(graph)\n\n\ndef protein_attributes(graph):\n    nx.set_node_attributes(graph, nx.betweenness_centrality(graph), \"betweenness\")\n\n     # Create a dictionary of attribute values for all nodes\n    attributes = {}\n    for node in graph.nodes:\n        att_dict = {\n            k: v[node] if node in v else 0. for k, v in gravy_scales.items()\n        }\n        att_dict.update({\n            'flexibility': Flex.get(node, 0.),\n            'hydrophilicity': hw.get(node, 0.),\n            'surface_accessibility': em.get(node, 0.),\n            'janin_interior': ja.get(node, 0.)\n        })\n        attributes[node] = att_dict\n\n    # Update attributes for all nodes in a single step\n    nx.set_node_attributes(graph, attributes)\n\n    # Now setup interaction details\n    #for v1,v2 in graph.edges:\n    #    try:\n    #        graph[v1][v2]['instability_index'] = DIWV[v1][v2]\n    #    except KeyError:\n    #        graph[v1][v2]['instability_index'] = 0.\n\n    return graph","metadata":{"id":"FI3rmj3H1K4X","execution":{"iopub.status.busy":"2023-07-19T04:15:00.310829Z","iopub.execute_input":"2023-07-19T04:15:00.311542Z","iopub.status.idle":"2023-07-19T04:15:00.322863Z","shell.execute_reply.started":"2023-07-19T04:15:00.311509Z","shell.execute_reply":"2023-07-19T04:15:00.321699Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Define a function to process a record and return the protein recurrence network\n#def process_record(record):\n#    return record.id, protein_recurrence_network(str(record.seq))\ndef process_record(record):\n    if record.id in train_terms_set:\n        return record.id, protein_recurrence_network(str(record.seq))\n    else:\n        return None\n\nif notebook_run:\n    train_terms_set = set(train_terms['EntryID'].values)\n    \n    # Parse the fasta file\n    records = SeqIO.parse(kaggle_input_data + \"/Train/train_sequences.fasta\", \"fasta\")\n\n    # Use map to process the records in parallel\n    #protein_rn_dict = dict(map(process_record, (record for record in records if record.id in train_terms['EntryID'].values)))\n    #g_train_protein_ids = np.array(list(protein_rn_dict.keys()))\n    \n    # Use concurrent.futures to process the records in parallel\n    with concurrent.futures.ProcessPoolExecutor() as executor:\n        future_to_record = {executor.submit(process_record, record): record for record in records}\n        protein_rn_dict = {future.result()[0]: future.result()[1] for future in concurrent.futures.as_completed(future_to_record) if future.result() is not None}\n        g_train_protein_ids = np.array(list(protein_rn_dict.keys()))","metadata":{"id":"VD5T4vMS1K4X","execution":{"iopub.status.busy":"2023-07-19T04:15:00.324660Z","iopub.execute_input":"2023-07-19T04:15:00.325145Z","iopub.status.idle":"2023-07-19T04:15:00.336365Z","shell.execute_reply.started":"2023-07-19T04:15:00.325112Z","shell.execute_reply":"2023-07-19T04:15:00.335305Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"Let's draw one of these graphs to see what it looks like.","metadata":{"id":"bD5sC7v01K4X"}},{"cell_type":"code","source":"if notebook_run:\n    G_id = train_terms['EntryID'].sample(1).values[0]\n    G = protein_rn_dict[G_id]\n    fig = plt.figure(figsize=(12,12))\n    # Extract edge weights\n    edge_weights = nx.get_edge_attributes(G, 'weight')\n    # Round the edge weights to 4 decimal places\n    edge_weights = {k: round(v, 4) for k, v in edge_weights.items()}\n\n    # Draw the graph\n    pos = nx.spring_layout(G)\n    nx.draw_networkx(G, pos, with_labels=True, node_color='lightblue')\n\n    # Draw edge labels\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_weights, font_color='red', font_size=8)\n    plt.title(G_id)\n\n    # Show the graph\n    plt.axis('off')\n    plt.show()","metadata":{"id":"BxPwR0if1K4X","colab":{"base_uri":"https://localhost:8080/","height":961},"outputId":"ae50251b-462b-4d18-f046-0816358ea0ca","execution":{"iopub.status.busy":"2023-07-19T04:15:00.337924Z","iopub.execute_input":"2023-07-19T04:15:00.338390Z","iopub.status.idle":"2023-07-19T04:15:00.349707Z","shell.execute_reply.started":"2023-07-19T04:15:00.338357Z","shell.execute_reply":"2023-07-19T04:15:00.348825Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"if notebook_run:\n    list(G.nodes(data=True))\n    #for v1,v2 in G.edges:\n    #  print(v1)\n    #  print(v2)","metadata":{"id":"Km07L70J4YFG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"014b1315-56af-4392-833b-7880a77cb349","scrolled":true,"execution":{"iopub.status.busy":"2023-07-19T04:15:00.351706Z","iopub.execute_input":"2023-07-19T04:15:00.352520Z","iopub.status.idle":"2023-07-19T04:15:00.358949Z","shell.execute_reply.started":"2023-07-19T04:15:00.352483Z","shell.execute_reply":"2023-07-19T04:15:00.357945Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"if notebook_run:\n    # Just looking for weird proteins\n    for k in protein_rn_dict:\n        if 'U' in protein_rn_dict[k].nodes():\n            print(k)\n            print(protein_rn_dict[k].nodes())\n            print(protein_rn_dict[k])","metadata":{"scrolled":true,"id":"f4XokvA01K4X","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f36baad6-6ce8-4669-ab92-263eb903cec6","execution":{"iopub.status.busy":"2023-07-19T04:15:00.364094Z","iopub.execute_input":"2023-07-19T04:15:00.364418Z","iopub.status.idle":"2023-07-19T04:15:00.369831Z","shell.execute_reply.started":"2023-07-19T04:15:00.364395Z","shell.execute_reply":"2023-07-19T04:15:00.368867Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"# Prepare the dataset\n\nReference: https://www.kaggle.com/code/alexandervc/baseline-multilabel-to-multitarget-binary","metadata":{"papermill":{"duration":0.009208,"end_time":"2023-05-09T08:30:32.825978","exception":false,"start_time":"2023-05-09T08:30:32.81677","status":"completed"},"tags":[],"id":"EJ6434191K4X"}},{"cell_type":"markdown","source":"First we will extract all the needed labels(`GO term ID`) from `train_terms.tsv` file. There are more than 40,000 labels. In order to simplify our model, we will choose the most frequent 1500 `GO term ID`s as labels.","metadata":{"papermill":{"duration":0.009674,"end_time":"2023-05-09T08:30:32.845065","exception":false,"start_time":"2023-05-09T08:30:32.835391","status":"completed"},"tags":[],"id":"6LjXf3Bp1K4X"}},{"cell_type":"markdown","source":"Let's plot the most frequent 100 `GO Term ID`s in `train_terms.tsv`.","metadata":{"papermill":{"duration":0.009238,"end_time":"2023-05-09T08:30:32.863785","exception":false,"start_time":"2023-05-09T08:30:32.854547","status":"completed"},"tags":[],"id":"PasOnoXy1K4Y"}},{"cell_type":"code","source":"if notebook_run:\n    # Select first 1500 values for plotting\n    plot_df = train_terms['term'].value_counts().iloc[:100]\n\n    figure, axis = plt.subplots(1, 1, figsize=(12, 6))\n\n    bp = sns.barplot(ax=axis, x=np.array(plot_df.index), y=plot_df.values)\n    bp.set_xticklabels(bp.get_xticklabels(), rotation=90, size = 6)\n    axis.set_title('Top 100 frequent GO term IDs')\n    bp.set_xlabel(\"GO term IDs\", fontsize = 12)\n    bp.set_ylabel(\"Count\", fontsize = 12)\n    plt.show()","metadata":{"papermill":{"duration":1.592489,"end_time":"2023-05-09T08:30:34.465912","exception":false,"start_time":"2023-05-09T08:30:32.873423","status":"completed"},"tags":[],"id":"23FUlWzw1K4Y","colab":{"base_uri":"https://localhost:8080/","height":607},"outputId":"6fb28ae9-631c-4286-b831-7987248df807","execution":{"iopub.status.busy":"2023-07-19T04:15:00.371182Z","iopub.execute_input":"2023-07-19T04:15:00.372135Z","iopub.status.idle":"2023-07-19T04:15:00.381544Z","shell.execute_reply.started":"2023-07-19T04:15:00.372092Z","shell.execute_reply":"2023-07-19T04:15:00.380860Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"We will now save the first 1500 most frequent GO term Ids into a list.","metadata":{"papermill":{"duration":0.010458,"end_time":"2023-05-09T08:30:34.487707","exception":false,"start_time":"2023-05-09T08:30:34.477249","status":"completed"},"tags":[],"id":"UbjLGFEK1K4Y"}},{"cell_type":"code","source":"if notebook_run:\n    # Set the limit for label\n    num_of_labels = 1500\n\n    # Take value counts in descending order and fetch first 1500 `GO term ID` as labels\n    labels = train_terms['term'].value_counts().index[:num_of_labels].tolist()","metadata":{"papermill":{"duration":0.523976,"end_time":"2023-05-09T08:30:35.021974","exception":false,"start_time":"2023-05-09T08:30:34.497998","status":"completed"},"tags":[],"id":"PePJVY871K4Y","execution":{"iopub.status.busy":"2023-07-19T04:15:00.382775Z","iopub.execute_input":"2023-07-19T04:15:00.383820Z","iopub.status.idle":"2023-07-19T04:15:00.391236Z","shell.execute_reply.started":"2023-07-19T04:15:00.383724Z","shell.execute_reply":"2023-07-19T04:15:00.390303Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"Next, we will create a new dataframe by filtering the train terms with the selected `GO Term ID`s.","metadata":{"papermill":{"duration":0.009833,"end_time":"2023-05-09T08:30:35.042088","exception":false,"start_time":"2023-05-09T08:30:35.032255","status":"completed"},"tags":[],"id":"rfoR7jaK1K4f"}},{"cell_type":"code","source":"if notebook_run:\n    # Fetch the train_terms data for the relevant labels only\n    train_terms_updated = train_terms.loc[train_terms['term'].isin(labels)]","metadata":{"papermill":{"duration":0.668657,"end_time":"2023-05-09T08:30:35.720953","exception":false,"start_time":"2023-05-09T08:30:35.052296","status":"completed"},"tags":[],"id":"QmbKQ-wX1K4g","execution":{"iopub.status.busy":"2023-07-19T04:15:00.392646Z","iopub.execute_input":"2023-07-19T04:15:00.393317Z","iopub.status.idle":"2023-07-19T04:15:00.400516Z","shell.execute_reply.started":"2023-07-19T04:15:00.393284Z","shell.execute_reply":"2023-07-19T04:15:00.399613Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"Let us plot the aspect values in the new **train_terms_updated** dataframe using a pie chart.","metadata":{"papermill":{"duration":0.009797,"end_time":"2023-05-09T08:30:35.741328","exception":false,"start_time":"2023-05-09T08:30:35.731531","status":"completed"},"tags":[],"id":"hs4Ndc0k1K4g"}},{"cell_type":"code","source":"if notebook_run:\n    pie_df = train_terms_updated['aspect'].value_counts()\n    palette_color = sns.color_palette('bright')\n    plt.pie(pie_df.values, labels=np.array(pie_df.index), colors=palette_color, autopct='%.0f%%')\n    plt.show()","metadata":{"papermill":{"duration":0.419624,"end_time":"2023-05-09T08:30:36.171193","exception":false,"start_time":"2023-05-09T08:30:35.751569","status":"completed"},"tags":[],"id":"F3UvKHwt1K4g","colab":{"base_uri":"https://localhost:8080/","height":406},"outputId":"702cad97-8748-4bd7-efa5-74bc3f17486c","execution":{"iopub.status.busy":"2023-07-19T04:15:00.402335Z","iopub.execute_input":"2023-07-19T04:15:00.402697Z","iopub.status.idle":"2023-07-19T04:15:00.409695Z","shell.execute_reply.started":"2023-07-19T04:15:00.402666Z","shell.execute_reply":"2023-07-19T04:15:00.408685Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"As you can see, majority of the `GO term Id`s have BPO(Biological Process Ontology) as their aspect.","metadata":{"papermill":{"duration":0.016642,"end_time":"2023-05-09T08:30:36.204943","exception":false,"start_time":"2023-05-09T08:30:36.188301","status":"completed"},"tags":[],"id":"AKV3JhgU1K4h"}},{"cell_type":"markdown","source":"Since this is a multi label classification problem, in the labels array we will denote the presence or absence of each Go Term Id for a protein id using a 1 or 0.\nFirst, we will create a numpy array `train_labels` of required size for the labels. To update the `train_labels` array with the appropriate values, we will loop through the label list.","metadata":{"id":"0sexUW1v1K4i"}},{"cell_type":"code","source":"if notebook_run:\n    # Setup progressbar settings.\n    # This is strictly for aesthetic.\n    bar = progressbar.ProgressBar(maxval=num_of_labels, \\\n        widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n\n    # Create an empty dataframe of required size for storing the labels,\n    # i.e, train_size x num_of_labels (142246 x 1500)\n    train_size = g_train_protein_ids.shape[0] # len(X)\n    train_labels = np.zeros((train_size, num_of_labels))\n\n    # Convert from numpy to pandas series for better handling\n    series_train_protein_ids = pd.Series(g_train_protein_ids)\n\n    # Generate a dict where key is label and value is the corresponding proteins\n    def get_label_proteins(label):\n        proteins = train_terms_updated[train_terms_updated['term'] == label]['EntryID'].unique()\n        return label, proteins\n\n    label_protein_dict = dict(map(get_label_proteins, labels))\n\n    # Loop through each label\n    for i, label in enumerate(labels):\n        # Fetch all the unique EntryId aka proteins related to the current label(GO term ID)\n        label_related_proteins = label_protein_dict[label]\n\n        # In the series_train_protein_ids pandas series, if a protein is related\n        # to the current label, then mark it as 1, else 0.\n        # Replace the ith column of train_labels with that pandas series.\n        train_labels[:,i] =  series_train_protein_ids.isin(label_related_proteins).astype(float)\n\n        # Progress bar percentage increase\n        bar.update(i+1)\n\n    # Notify the end of progress bar\n    bar.finish()\n\n    # Convert train_labels numpy into pandas dataframe\n    labels_df = pd.DataFrame(data = train_labels, columns = labels)\n    print(labels_df.shape)\n","metadata":{"id":"qM8RdXsyDoDQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"27e9aff0-3c75-4976-8c59-221ef156dbaf","execution":{"iopub.status.busy":"2023-07-19T04:15:00.411445Z","iopub.execute_input":"2023-07-19T04:15:00.411817Z","iopub.status.idle":"2023-07-19T04:15:00.422402Z","shell.execute_reply.started":"2023-07-19T04:15:00.411765Z","shell.execute_reply":"2023-07-19T04:15:00.421608Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"The final labels dataframe (`label_df`) is composed of 1500 columns and 142246 entries (though we brought it down to 10000). We can see all 1500 dimensions(results will be truncated since the number of columns is big) of our dataset by printing out the first 5 entries using the following code:","metadata":{"papermill":{"duration":0.010097,"end_time":"2023-05-09T08:38:51.971947","exception":false,"start_time":"2023-05-09T08:38:51.96185","status":"completed"},"tags":[],"id":"nrQKV4Cn1K4j"}},{"cell_type":"code","source":"if notebook_run:\n    labels_df.head()","metadata":{"papermill":{"duration":0.048128,"end_time":"2023-05-09T08:38:52.031041","exception":false,"start_time":"2023-05-09T08:38:51.982913","status":"completed"},"tags":[],"id":"KZeiIuqY1K4k","colab":{"base_uri":"https://localhost:8080/","height":299},"outputId":"515c6879-e3d7-4a6b-cef6-68f20eeae8e0","execution":{"iopub.status.busy":"2023-07-19T04:15:00.423917Z","iopub.execute_input":"2023-07-19T04:15:00.424633Z","iopub.status.idle":"2023-07-19T04:15:00.435023Z","shell.execute_reply.started":"2023-07-19T04:15:00.424590Z","shell.execute_reply":"2023-07-19T04:15:00.434237Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"if notebook_run:\n    num_of_labels","metadata":{"id":"bIl4xgNi1K4k","colab":{"base_uri":"https://localhost:8080/"},"outputId":"98c17290-242a-4782-e829-f33f7475870e","execution":{"iopub.status.busy":"2023-07-19T04:15:00.436512Z","iopub.execute_input":"2023-07-19T04:15:00.437261Z","iopub.status.idle":"2023-07-19T04:15:00.445372Z","shell.execute_reply.started":"2023-07-19T04:15:00.437227Z","shell.execute_reply":"2023-07-19T04:15:00.444725Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"## Graph Preparation\n\nNext, we'll define our GNN model class, which will inherit from nn.Module:","metadata":{"id":"oIKSnaYQ1K4l"}},{"cell_type":"code","source":"class GraphSAGE(torch.nn.Module):\n    def __init__(self, in_channels=1, hidden_dim = 128, out_channels=1500):\n        \"\"\"\n        Represents a 3-layer GraphSAGE GNN model\n        with embedding and hidden dimension of hidden_dim.\n        \"\"\"\n        super(GraphSAGE, self).__init__()\n        self.tag = 'GraphSAGE'\n        self.pre_embs = nn.Embedding(in_channels, hidden_dim)\n\n        self.convs = nn.ModuleList()\n        self.selus = nn.ModuleList()\n        self.dropouts = nn.ModuleList()\n        self.layer_norms = nn.ModuleList()\n\n        # Input layer\n        self.convs.append(SAGEConv(in_channels, hidden_dim))\n        self.selus.append(nn.SELU())\n        self.dropouts.append(nn.Dropout(0.1))\n        self.layer_norms.append(nn.LayerNorm(hidden_dim))\n\n        # Hidden layers\n        for _ in range(3):\n            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n            self.selus.append(SELU())\n            self.dropouts.append(nn.Dropout(0.1))\n            self.layer_norms.append(nn.LayerNorm(hidden_dim))\n\n        # Output layer\n        self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n        self.out = nn.Linear(3*hidden_dim, out_channels)\n        self.out.weight.data.fill_(1.)\n        self.dropout = nn.Dropout(0.1)\n            \n    def forward(self, data):\n        \"\"\"\n        Runs a forward pass through GraphSAGE with given initial skill IDs and\n        edge_index and edge_weights.\n\n        Arguments:\n          - x: amino acid IDs (torch.Tensor)\n          - edge_index: edges in skill graph (torch.Tensor)\n          - edge_weight: edge weights of skill graph (torch.Tensor)\n\n        Returns:\n          - final node embedding for skill\n        \"\"\"\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        # Convert x tensor to Long type\n        #x = x.to(torch.long)\n        #x = self.pre_embs(x)\n        for conv, selu, dropout, layer_norm in zip(self.convs, self.selus, self.dropouts, self.layer_norms):\n          x = layer_norm(dropout(selu(conv(x, edge_index))))\n\n        x_add = global_add_pool(x, batch)\n        x_mean = global_mean_pool(x, batch)\n        x_max = global_max_pool(x, batch)\n        x = torch.cat([x_add, x_mean, x_max], dim=-1)\n        return self.out(x)\n\nclass GCN(torch.nn.Module):\n    def __init__(self, in_channels, dim, out_channels):\n        super(GCN, self).__init__()\n        torch.manual_seed(12345)\n        self.conv1 = GCNConv(1, hidden_channels)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n        self.lin = Linear(hidden_channels, num_classes)\n\n    def forward(self, x, edge_index, batch):\n        # 1. Obtain node embeddings\n        x = self.conv1(x, edge_index)\n        x = F.selu(x)\n        x = self.conv2(x, edge_index)\n        x = F.selu(x)\n        x = self.conv3(x, edge_index)\n\n        # 2. Readout layer\n        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n\n        # 3. Apply a final classifier\n        x = F.dropout(x, p=0.1, training=self.training)\n        x = self.lin(x)\n\n        return x\n","metadata":{"scrolled":true,"id":"5lrHQvSH1K4l","execution":{"iopub.status.busy":"2023-07-19T04:15:00.446867Z","iopub.execute_input":"2023-07-19T04:15:00.447459Z","iopub.status.idle":"2023-07-19T04:15:00.473281Z","shell.execute_reply.started":"2023-07-19T04:15:00.447426Z","shell.execute_reply":"2023-07-19T04:15:00.472299Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"Here, **hidden_dim** is the dimensionality of hidden layers, and **output_dim** is the number of output classes (1500 in this case).\n\nNow, let's define a function to convert each NetworkX graph into a PyTorch Geometric **Data** object:","metadata":{"id":"BwaU1rn01K4n"}},{"cell_type":"code","source":"# Assuming nx_graphs_train and nx_graphs_test are your lists of NetworkX graphs\n# and labels_train and labels_test are your lists of labels for train and test datasets respectively\n#TODO Use map for this and do it with singles\ndef from_networkx_to_data(nx_graph, labels):\n    data = from_networkx(nx_graph)\n\n    # Extract node properties and convert them to a feature tensor\n    data.x = torch.tensor([[float(nx_graph.nodes[node][prop]) for prop in nx_graph.nodes[node]]\n                           for node in nx_graph.nodes()], dtype=torch.float)\n    \n    data.y = torch.tensor(labels, dtype=torch.float).unsqueeze(0)\n    return data\n\ndef from_networkx_to_data_list(nx_graphs, labels):\n    return list(map(from_networkx_to_data, nx_graphs,labels))","metadata":{"id":"qTOl5aMC1K4n","execution":{"iopub.status.busy":"2023-07-19T05:38:23.003357Z","iopub.execute_input":"2023-07-19T05:38:23.004076Z","iopub.status.idle":"2023-07-19T05:38:23.013174Z","shell.execute_reply.started":"2023-07-19T05:38:23.004021Z","shell.execute_reply":"2023-07-19T05:38:23.012121Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"In this function, we convert the adjacency matrix of the graph into the edge index format expected by PyTorch Geometric. We also convert the labels into a torch tensor.\n\n# Training\n\nNow, let's define the training loop and train the GNN model on your labeled graphs:","metadata":{"id":"FpwTzGAl1K4o"}},{"cell_type":"code","source":"import random\nfrom datetime import datetime\nrandom.seed(datetime.now().timestamp())\nif notebook_run:\n    # Assuming you have a list of NetworkX graphs and corresponding labels\n\n    #protein_ids = list(protein_rn_dict.keys())\n    protein_ids = g_train_protein_ids\n    num_features = 33\n    num_classes = 1500\n    graphs, graphs_test, labels, labels_test = train_test_split([protein_rn_dict[k] for k in protein_ids],\n                                                                labels_df.iloc[np.where(np.isin(g_train_protein_ids, protein_ids))].values,\n                                                                test_size=0.2, random_state=int(datetime.now().timestamp()))\n\n    # Convert each graph to a PyTorch Geometric Data object and place in data loader\n    train_loader = DataLoader(from_networkx_to_data_list(graphs, labels), batch_size=256, shuffle=True)\n    test_loader = DataLoader(from_networkx_to_data_list(graphs_test, labels_test), batch_size=256, shuffle=False)","metadata":{"id":"VBPSIOJm1K4o","colab":{"base_uri":"https://localhost:8080/"},"outputId":"134e042a-31b8-4537-8157-7a576463b3ef","execution":{"iopub.status.busy":"2023-07-19T05:38:23.016000Z","iopub.execute_input":"2023-07-19T05:38:23.016867Z","iopub.status.idle":"2023-07-19T05:38:23.025765Z","shell.execute_reply.started":"2023-07-19T05:38:23.016831Z","shell.execute_reply":"2023-07-19T05:38:23.024616Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"We'll introduce a method to do all of this data preparation at once, such that we don't store all of these intermediate variables in memory.","metadata":{}},{"cell_type":"code","source":"class ProteinDataset(Dataset):\n    def __init__(self, protein_sequences, labels=torch.zeros((1500,1)), training=False):\n        self.protein_sequences = protein_sequences\n        self.labels = labels\n        self.training = training\n        if training:\n            #self.nets = [protein_recurrence_network(seq) for seq in protein_sequences]\n            self.nets = list(map(protein_recurrence_network, protein_sequences))\n    def __len__(self):\n        return len(self.protein_sequences)\n\n    def __getitem__(self, idx):\n        sequence = self.protein_sequences[idx]\n        # Convert the NetworkX graph to a PyTorch Geometric Data object\n        if self.training:\n            data = from_networkx_to_data(self.nets[idx], self.labels[idx])\n        else:\n            data = from_networkx_to_data(protein_recurrence_network(sequence), self.labels[idx])\n        return data\n\ndef prepare_training_data_and_models(sample_data_size=150000, num_of_labels=1500, training=True, just_data=False):\n    '''\n        Inputs:\n            sample_data_size - How many samples should be used in the training set\n            num_of_labels - Limit of how many lables (most frequently occurring) should we include\n    '''\n    train_terms = pd.read_csv(kaggle_input_data + \"/Train/train_terms.tsv\",sep=\"\\t\")\n    print(\"Training terms shape: \")\n    print(train_terms.shape)\n    if sample_data_size:\n        train_terms = train_terms.sample(sample_data_size)\n\n    print(\"Getting record IDs\")\n    # Parse the fasta file\n    records = SeqIO.parse(kaggle_input_data + \"/Train/train_sequences.fasta\", \"fasta\")\n    entry_id_set = set(train_terms['EntryID'].values)\n    train_protein_ids = np.array([record.id for record in records if record.id in entry_id_set])\n    print(str(len(train_protein_ids)) + \" Protein IDs found\")\n\n    print(\"Fetching Label Values\")\n    # Take value counts in descending order and fetch first 1500 `GO term ID` as labels\n    labels = train_terms['term'].value_counts().index[:num_of_labels].tolist()\n    \n    # Fetch the train_terms data for the relevant labels only\n    train_terms_updated = train_terms.loc[train_terms['term'].isin(labels)]\n    \n    print(\"Establishing labels\")\n    \n    # Setup progressbar settings.\n    # This is strictly for aesthetic.\n    bar = progressbar.ProgressBar(maxval=num_of_labels, \\\n        widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n\n    # Create an empty dataframe of required size for storing the labels,\n    # i.e, train_size x num_of_labels (142246 x 1500)\n    train_size = train_protein_ids.shape[0] # len(X)\n    train_labels = np.zeros((train_size, num_of_labels))\n\n    # Convert from numpy to pandas series for better handling\n    series_train_protein_ids = pd.Series(train_protein_ids)\n\n    # Generate a dict where key is label and value is the corresponding proteins\n    def get_label_proteins(label):\n        proteins = train_terms_updated[train_terms_updated['term'] == label]['EntryID'].unique()\n        return label, proteins\n    \n    print(\"Creating Label-Protein Dictionary\")\n\n    label_protein_dict = dict(map(get_label_proteins, labels))\n\n    # Loop through each label\n    for i, label in enumerate(labels):\n        # Fetch all the unique EntryId aka proteins related to the current label(GO term ID)\n        label_related_proteins = label_protein_dict[label]\n\n        # In the series_train_protein_ids pandas series, if a protein is related\n        # to the current label, then mark it as 1, else 0.\n        # Replace the ith column of train_labels with that pandas series.\n        train_labels[:,i] =  series_train_protein_ids.isin(label_related_proteins).astype(float)\n\n        # Progress bar percentage increase\n        bar.update(i+1)\n\n    # Notify the end of progress bar\n    bar.finish()\n\n    # Convert train_labels numpy into pandas dataframe\n    labels_df = pd.DataFrame(data = train_labels, columns = labels)\n    \n    #protein_ids = list(protein_rn_dict.keys())\n    protein_ids = train_protein_ids\n    num_features = 33\n    num_classes = num_of_labels\n    proteins_train, proteins_test, labels, labels_test = train_test_split(protein_ids,\n                                                                labels_df.iloc[np.where(np.isin(train_protein_ids, protein_ids))].values,\n                                                                test_size=0.2, random_state=int(datetime.now().timestamp()))\n    print(\"Getting sequences for training\")\n    records = SeqIO.parse(kaggle_input_data + \"/Train/train_sequences.fasta\", \"fasta\")\n    proteins_train_set = set(proteins_train)\n    training_dataset = ProteinDataset([str(record.seq) for record in records if record.id in proteins_train_set], labels, training=training)\n    print(\"Getting sequences for testing\")\n    records = SeqIO.parse(kaggle_input_data + \"/Train/train_sequences.fasta\", \"fasta\")\n    proteins_test_set = set(proteins_test)\n    test_dataset = ProteinDataset([str(record.seq) for record in records if record.id in proteins_test_set], labels_test, training=training)\n    \n    print(\"Putting training data into Data Loader\")\n    # Convert each graph to a PyTorch Geometric Data object and place in data loader\n    train_loader = DataLoader(training_dataset, batch_size=2048, shuffle=True, num_workers=2, pin_memory=True)\n    print(\"Putting test data into Data Loader\")\n    test_loader = DataLoader(test_dataset, batch_size=2048, shuffle=False, num_workers=2, pin_memory=True)\n    \n    # Prepare Model\n    print(\"Preparing model\")\n    def weights_init(m):\n        if isinstance(m, nn.Linear):\n            m.weight.data.fill_(1.)\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    #model = GINNet(num_features, 256, num_classes).to(device)\n    model = GAT(in_channels=num_features,hidden_channels=128,num_layers=5,out_channels=num_classes,dropout=0.1,act='selu').to(device)\n    #model = GraphSAGE(num_features, 128, num_classes).to(device)\n    # Initialize final layer to 0s\n    model.apply(weights_init)\n    hidden_channels = 64\n    num_layers = 30\n    #model = GCN(num_features, 256, num_classes).to(device)\n    #model = GraphUnet(num_features, num_classes, n_hidden=128)\n    #model = models.GraphSAGE(num_features,hidden_channels,num_layers,num_classes,dropout=0.1,act='selu',act_first=True).to(device)\n    if just_data:\n        return train_loader, test_loader\n    else:\n        return train_loader, test_loader, model, device","metadata":{"execution":{"iopub.status.busy":"2023-07-19T05:50:15.393641Z","iopub.execute_input":"2023-07-19T05:50:15.394272Z","iopub.status.idle":"2023-07-19T05:50:15.428253Z","shell.execute_reply.started":"2023-07-19T05:50:15.394230Z","shell.execute_reply":"2023-07-19T05:50:15.426779Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"if not notebook_run:\n    train_loader, test_loader, model, device = prepare_training_data_and_models(sample_data_size=20000)","metadata":{"execution":{"iopub.status.busy":"2023-07-19T05:50:15.430898Z","iopub.execute_input":"2023-07-19T05:50:15.431415Z","iopub.status.idle":"2023-07-19T05:51:35.466302Z","shell.execute_reply.started":"2023-07-19T05:50:15.431373Z","shell.execute_reply":"2023-07-19T05:51:35.465073Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Training terms shape: \n(5363863, 3)\nGetting record IDs\n17306 Protein IDs found\nFetching Label Values\nEstablishing labels\nCreating Label-Protein Dictionary\n","output_type":"stream"},{"name":"stderr","text":"[========================================================================] 100%\n","output_type":"stream"},{"name":"stdout","text":"Getting sequences for training\nGetting sequences for testing\nPutting training data into Data Loader\nPutting test data into Data Loader\nPreparing model\n","output_type":"stream"}]},{"cell_type":"code","source":"for i,data in enumerate(train_loader):\n    print(f'Step {i + 1}:')\n    print('=======')\n    print(f'Number of graphs in the current batch: {data.num_graphs}')\n    print(data)\n    print()\n    \n    if i > 10:\n        break","metadata":{"scrolled":true,"id":"oZkqGbCD1K4p","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f5e72260-62c8-4f50-e8b0-ce4a48944faa","execution":{"iopub.status.busy":"2023-07-19T05:51:35.468921Z","iopub.execute_input":"2023-07-19T05:51:35.469333Z","iopub.status.idle":"2023-07-19T05:52:13.749192Z","shell.execute_reply.started":"2023-07-19T05:51:35.469297Z","shell.execute_reply":"2023-07-19T05:52:13.747751Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"Step 1:\n=======\nNumber of graphs in the current batch: 2048\nDataBatch(edge_index=[2, 463841], betweenness=[40536], KyteDoolitle=[40536], Aboderin=[40536], AbrahamLeo=[40536], Argos=[40536], BlackMould=[40536], BullBreese=[40536], Casari=[40536], Cid=[40536], Cowan3.4=[40536], Cowan7.5=[40536], Eisenberg=[40536], Engelman=[40536], Fasman=[40536], Fauchere=[40536], GoldSack=[40536], Guy=[40536], Jones=[40536], Juretic=[40536], Kidera=[40536], Miyazawa=[40536], Parker=[40536], Ponnuswamy=[40536], Rose=[40536], Roseman=[40536], Sweet=[40536], Tanford=[40536], Wilson=[40536], Zimmerman=[40536], flexibility=[40536], hydrophilicity=[40536], surface_accessibility=[40536], janin_interior=[40536], weight=[463841], num_nodes=40536, x=[40536, 33], y=[2048, 1500], batch=[40536], ptr=[2049])\n\nStep 2:\n=======\nNumber of graphs in the current batch: 2048\nDataBatch(edge_index=[2, 466078], betweenness=[40557], KyteDoolitle=[40557], Aboderin=[40557], AbrahamLeo=[40557], Argos=[40557], BlackMould=[40557], BullBreese=[40557], Casari=[40557], Cid=[40557], Cowan3.4=[40557], Cowan7.5=[40557], Eisenberg=[40557], Engelman=[40557], Fasman=[40557], Fauchere=[40557], GoldSack=[40557], Guy=[40557], Jones=[40557], Juretic=[40557], Kidera=[40557], Miyazawa=[40557], Parker=[40557], Ponnuswamy=[40557], Rose=[40557], Roseman=[40557], Sweet=[40557], Tanford=[40557], Wilson=[40557], Zimmerman=[40557], flexibility=[40557], hydrophilicity=[40557], surface_accessibility=[40557], janin_interior=[40557], weight=[466078], num_nodes=40557, x=[40557, 33], y=[2048, 1500], batch=[40557], ptr=[2049])\n\nStep 3:\n=======\nNumber of graphs in the current batch: 2048\nDataBatch(edge_index=[2, 459590], betweenness=[40561], KyteDoolitle=[40561], Aboderin=[40561], AbrahamLeo=[40561], Argos=[40561], BlackMould=[40561], BullBreese=[40561], Casari=[40561], Cid=[40561], Cowan3.4=[40561], Cowan7.5=[40561], Eisenberg=[40561], Engelman=[40561], Fasman=[40561], Fauchere=[40561], GoldSack=[40561], Guy=[40561], Jones=[40561], Juretic=[40561], Kidera=[40561], Miyazawa=[40561], Parker=[40561], Ponnuswamy=[40561], Rose=[40561], Roseman=[40561], Sweet=[40561], Tanford=[40561], Wilson=[40561], Zimmerman=[40561], flexibility=[40561], hydrophilicity=[40561], surface_accessibility=[40561], janin_interior=[40561], weight=[459590], num_nodes=40561, x=[40561, 33], y=[2048, 1500], batch=[40561], ptr=[2049])\n\nStep 4:\n=======\nNumber of graphs in the current batch: 2048\nDataBatch(edge_index=[2, 461935], betweenness=[40550], KyteDoolitle=[40550], Aboderin=[40550], AbrahamLeo=[40550], Argos=[40550], BlackMould=[40550], BullBreese=[40550], Casari=[40550], Cid=[40550], Cowan3.4=[40550], Cowan7.5=[40550], Eisenberg=[40550], Engelman=[40550], Fasman=[40550], Fauchere=[40550], GoldSack=[40550], Guy=[40550], Jones=[40550], Juretic=[40550], Kidera=[40550], Miyazawa=[40550], Parker=[40550], Ponnuswamy=[40550], Rose=[40550], Roseman=[40550], Sweet=[40550], Tanford=[40550], Wilson=[40550], Zimmerman=[40550], flexibility=[40550], hydrophilicity=[40550], surface_accessibility=[40550], janin_interior=[40550], weight=[461935], num_nodes=40550, x=[40550, 33], y=[2048, 1500], batch=[40550], ptr=[2049])\n\nStep 5:\n=======\nNumber of graphs in the current batch: 2048\nDataBatch(edge_index=[2, 461694], betweenness=[40490], KyteDoolitle=[40490], Aboderin=[40490], AbrahamLeo=[40490], Argos=[40490], BlackMould=[40490], BullBreese=[40490], Casari=[40490], Cid=[40490], Cowan3.4=[40490], Cowan7.5=[40490], Eisenberg=[40490], Engelman=[40490], Fasman=[40490], Fauchere=[40490], GoldSack=[40490], Guy=[40490], Jones=[40490], Juretic=[40490], Kidera=[40490], Miyazawa=[40490], Parker=[40490], Ponnuswamy=[40490], Rose=[40490], Roseman=[40490], Sweet=[40490], Tanford=[40490], Wilson=[40490], Zimmerman=[40490], flexibility=[40490], hydrophilicity=[40490], surface_accessibility=[40490], janin_interior=[40490], weight=[461694], num_nodes=40490, x=[40490, 33], y=[2048, 1500], batch=[40490], ptr=[2049])\n\nStep 6:\n=======\nNumber of graphs in the current batch: 2048\nDataBatch(edge_index=[2, 465409], betweenness=[40526], KyteDoolitle=[40526], Aboderin=[40526], AbrahamLeo=[40526], Argos=[40526], BlackMould=[40526], BullBreese=[40526], Casari=[40526], Cid=[40526], Cowan3.4=[40526], Cowan7.5=[40526], Eisenberg=[40526], Engelman=[40526], Fasman=[40526], Fauchere=[40526], GoldSack=[40526], Guy=[40526], Jones=[40526], Juretic=[40526], Kidera=[40526], Miyazawa=[40526], Parker=[40526], Ponnuswamy=[40526], Rose=[40526], Roseman=[40526], Sweet=[40526], Tanford=[40526], Wilson=[40526], Zimmerman=[40526], flexibility=[40526], hydrophilicity=[40526], surface_accessibility=[40526], janin_interior=[40526], weight=[465409], num_nodes=40526, x=[40526, 33], y=[2048, 1500], batch=[40526], ptr=[2049])\n\nStep 7:\n=======\nNumber of graphs in the current batch: 1556\nDataBatch(edge_index=[2, 353947], betweenness=[30859], KyteDoolitle=[30859], Aboderin=[30859], AbrahamLeo=[30859], Argos=[30859], BlackMould=[30859], BullBreese=[30859], Casari=[30859], Cid=[30859], Cowan3.4=[30859], Cowan7.5=[30859], Eisenberg=[30859], Engelman=[30859], Fasman=[30859], Fauchere=[30859], GoldSack=[30859], Guy=[30859], Jones=[30859], Juretic=[30859], Kidera=[30859], Miyazawa=[30859], Parker=[30859], Ponnuswamy=[30859], Rose=[30859], Roseman=[30859], Sweet=[30859], Tanford=[30859], Wilson=[30859], Zimmerman=[30859], flexibility=[30859], hydrophilicity=[30859], surface_accessibility=[30859], janin_interior=[30859], weight=[353947], num_nodes=30859, x=[30859, 33], y=[1556, 1500], batch=[30859], ptr=[1557])\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import clear_output\n\nif notebook_run:\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    #model = GINNet(num_features, 256, num_classes).to(device)\n    model = GraphSAGE(num_features, 256, num_classes).to(device)\n\n    hidden_channels = 64\n    num_layers = 30\n    #model = GCN(num_features, 256, num_classes).to(device)\n    #model = GraphUnet(num_features, num_classes, n_hidden=128)\n    #model = models.GraphSAGE(num_features,hidden_channels,num_layers,num_classes,dropout=0.1,act='selu',act_first=True).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\nloss_function = torch.nn.CrossEntropyLoss()\n\ndef train():\n    model.train()\n\n    total_loss = 0\n    accumulation_steps = 1  # Change to fit your needs\n    optimizer.zero_grad()  # Initialize gradients to zero at the start of each epoch\n\n    #i = 0\n    for i,data in enumerate(train_loader):\n        data = data.to(device)\n        #print(data.x)\n        out = model(x=data.x,edge_index=data.edge_index,edge_weight=data.weight)\n        #out = model(data.x,data.edge_index)\n        #print(out)\n        target = data.y.argmax(dim=1)\n        #print(target)\n        #target_indices = torch.argmax(target, dim=1)\n        loss = loss_function(out, target)\n        loss.backward()  # Backward pass (calculate gradients)\n\n        #if (i+1) % accumulation_steps == 0:  # Perform the optimization step every accumulation_steps\n        optimizer.step()\n        optimizer.zero_grad()\n        total_loss += loss.item() * data.num_graphs\n        #i += 1\n\n    # After the training loop ends, there might be gradients that are not yet updated\n    # So, you should perform an optimization step outside the loop to update those gradients\n    optimizer.step()\n\n    return total_loss / (i+1)  # Here i+1 is the number of optimizer steps\n\ndef test(data_loader, verbose=True):\n    model.eval()\n    all_preds = []\n    all_labels = []\n    total_loss = 0\n\n    with torch.no_grad():\n        for data in data_loader:\n            data = data.to(device)\n            out = model(x=data.x,edge_index=data.edge_index,edge_weight=data.weight)\n            #out = model(data.x,data.edge_index, edge_weight=data.weight, edge_attr=data.instability_index)\n            pred = out.argmax(dim=1).cpu().numpy()  # get class predictions\n            all_preds.append(pred)\n\n            labels = torch.argmax(data.y, dim=1)  # get true labels\n            all_labels.append(labels.cpu().numpy())\n            total_loss += loss_function(out, labels).item()  # sum up batch loss\n\n\n    all_preds = np.concatenate(all_preds, axis=0)\n    all_labels = np.concatenate(all_labels, axis=0)\n    clear_output(wait=True)\n    if verbose:\n        print(classification_report(all_labels, all_preds))\n    return total_loss / len(data_loader.dataset)\n\nnum_folds = 5\nnum_epochs = 100\nlosses = []\ntest_accs = []\nepochs = []\n\nfor k in range(num_folds):\n    print(\"Fold: \" + str(k+1))\n    for epoch in range(1, num_epochs):\n        loss = train()\n        test_acc = test(test_loader)\n\n        # Update the loss, test_acc, and epochs variables\n        losses.append(loss)\n        test_accs.append(test_acc)\n        epochs.append(epoch)\n\n        #print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Test Acc: {test_acc:.4f}')\n        # Plot the updated data\n        plt.plot(epochs, losses, label='Loss')\n        plt.plot(epochs, test_accs, label='Test Accuracy')\n\n        # Add labels and a legend to the plot\n        plt.xlabel('Epoch')\n        plt.ylabel('Value')\n        plt.legend()\n\n        # Label the last loss and test_acc values\n        last_loss = round(losses[-1], 4)\n        last_acc = round(test_accs[-1], 4)\n        plt.text(epochs[-1], losses[-1], f'{last_loss}', ha='right', va='bottom')\n        plt.text(epochs[-1], test_accs[-1], f'{last_acc}', ha='right', va='bottom')\n\n        print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Test Acc: {test_acc:.4f}')\n        # Show the plot\n        plt.show()\n        scheduler.step()\n    \n    train_loader, test_loader = prepare_training_data_and_models(sample_data_size=20000, just_data=True)\n\ntorch.save(model, 'gcm.torch')\ndel train_loader\ndel test_loader","metadata":{"id":"AykYOE5u1K4p","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d782a1f0-728a-415e-ca0d-1313c4b42760","scrolled":true,"execution":{"iopub.status.busy":"2023-07-19T05:58:32.662483Z","iopub.execute_input":"2023-07-19T05:58:32.662947Z","iopub.status.idle":"2023-07-19T05:58:56.102336Z","shell.execute_reply.started":"2023-07-19T05:58:32.662909Z","shell.execute_reply":"2023-07-19T05:58:56.100632Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Fold: 1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[76], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFold: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_epochs):\n\u001b[0;32m---> 85\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m test(test_loader)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Update the loss, test_acc, and epochs variables\u001b[39;00m\n","Cell \u001b[0;32mIn[76], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#print(data.x)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43medge_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#out = model(data.x,data.edge_index)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#print(out)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_geometric/nn/models/basic_gnn.py:224\u001b[0m, in \u001b[0;36mBasicGNN.forward\u001b[0;34m(self, x, edge_index, edge_weight, edge_attr, num_sampled_nodes_per_hop, num_sampled_edges_per_hop)\u001b[0m\n\u001b[1;32m    222\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs[i](x, edge_index, edge_weight\u001b[38;5;241m=\u001b[39medge_weight)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_edge_attr:\n\u001b[0;32m--> 224\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs[i](x, edge_index)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_geometric/nn/conv/gat_conv.py:255\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    252\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_updater(edge_index, alpha\u001b[38;5;241m=\u001b[39malpha, edge_attr\u001b[38;5;241m=\u001b[39medge_attr)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[0;32m--> 255\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat:\n\u001b[1;32m    258\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:459\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m decomp_args:\n\u001b[1;32m    457\u001b[0m         kwargs[arg] \u001b[38;5;241m=\u001b[39m decomp_kwargs[arg][i]\n\u001b[0;32m--> 459\u001b[0m coll_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_user_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m msg_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minspector\u001b[38;5;241m.\u001b[39mdistribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m, coll_dict)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_forward_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:336\u001b[0m, in \u001b[0;36mMessagePassing._collect\u001b[0;34m(self, args, edge_index, size, kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Tensor):\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_size(size, dim, data)\n\u001b[0;32m--> 336\u001b[0m             data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m         out[arg] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_sparse_tensor(edge_index):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:282\u001b[0m, in \u001b[0;36mMessagePassing._lift\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered an index error. Please ensure that all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m point to valid indices in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(got interval \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmin())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(index\u001b[38;5;241m.\u001b[39mmax())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m])\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound negative indices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Please ensure that all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m point to valid indices \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the interval [0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour node feature matrix and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:272\u001b[0m, in \u001b[0;36mMessagePassing._lift\u001b[0;34m(self, src, edge_index, dim)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m     index \u001b[38;5;241m=\u001b[39m edge_index[dim]\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m index\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_dim):\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.69 GiB (GPU 0; 15.90 GiB total capacity; 10.92 GiB already allocated; 1.62 GiB free; 13.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 2.69 GiB (GPU 0; 15.90 GiB total capacity; 10.92 GiB already allocated; 1.62 GiB free; 13.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"markdown","source":"# Evaluate\n\nNow, we write some basic code to take a Networkx graph and predict the classes.","metadata":{"id":"jmEIO7Bj1K4r"}},{"cell_type":"code","source":"def predict_mult(ids, sequences, verbose=False):    \n    protein_dataset = ProteinDataset(sequences, torch.zeros((len(ids),1500,1)), training=False)\n    loader = DataLoader(protein_dataset, batch_size=1024, shuffle=False)\n\n    if verbose:\n        print(\"Evaluating sequences\")\n        print(\"Total sequences: \" + str(len(ids)))\n        \n    # Set the model to evaluation mode\n    model.eval()\n    predictions = []  # initialize an empty list to collect all predictions\n    \n    # Forward pass through the model\n    with torch.no_grad():\n        \n        for i,batch in enumerate(loader):\n            batch = batch.to(device)  # move batch to the device (GPU or CPU)\n            outputs = model(batch)  # forward pass\n            # process the outputs (e.g., apply a threshold in case of binary classification, etc.)\n            batch_predictions = (outputs > 0.5).float()\n            if verbose:\n                print(\"Batch output \" + str(i) + \": \")\n                print(batch_predictions)\n            # Detach the predictions from the computation graph and convert to a numpy array.\n            # Then add them to the list of all predictions.\n            predictions.extend(batch_predictions.cpu().numpy().tolist())\n\n        if verbose:\n            print(\"Model prediction: \")\n            print(predictions)\n        #output = model(x=data.x, edge_index=data.edge_index, edge_weight=edge_weight)\n\n    return ids, predictions\n\ndef predict(sequence, verbose=False):\n    g = protein_recurrence_network(sequence)\n    # Convert the NetworkX graph to a PyTorch Geometric Data object\n    data = from_networkx_to_data(g, torch.zeros((1500,1)))\n    \n    if verbose:\n        print(\"Data: \")\n        print(data)\n\n    # Move data to the appropriate device\n    data = data.to(device)\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Forward pass through the model\n    with torch.no_grad():\n        output = model(data)\n        if verbose:\n            print(\"Model prediction: \")\n            print(output)\n        #output = model(x=data.x, edge_index=data.edge_index, edge_weight=edge_weight)\n\n    # The output is log-probabilities, use softmax to get probabilities\n    out_probs = torch.sigmoid(output)\n    \n    # In a multi-label classification, an element is considered to be predicted as '1'\n    # if its probability is greater than or equal to 0.5.\n    out_preds = (out_probs > 0.5).float()\n\n    return out_preds","metadata":{"id":"x6g52g-i1K4s","execution":{"iopub.status.busy":"2023-07-19T05:52:37.238969Z","iopub.status.idle":"2023-07-19T05:52:37.239913Z","shell.execute_reply.started":"2023-07-19T05:52:37.239640Z","shell.execute_reply":"2023-07-19T05:52:37.239665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_seq = 'YAWVHDHCNWCLERVGTVHDEKWTMEDPMVPKHCECNEPIQAWTQDNDLYNFCITLQICANNNRDGGNLIGRVDQLALQKRLLDQNWHKNCTPPSVVTRCNCATCEEKVETRVHFMKIMGESGWDGWVMYLPLGQQICYSIPHRQHKCWWSIWFVDFRLLPIGERNDTLSCFMIIKLEWIFDVCHLHDKSIEIAEAGVQIQVWAQQICSGTYIEGWLDWENSPIACPDGDYWSNWTAMVVAFCECRKRMCSVIPQTKFYAFHMVHNKWWLQWFYTSHEGKKIVGIEYHQGSCYYKWTKGEKHMHMNVEQRQWGADQVVHTPFNAWACWMHTAKHGDCNVPGQHWGMGWFRDDDELAAYGHHTEHGDATQLLFGTCYVCPLNIVYLMGTLVLPRHQHVRNKRPMVDRDMVYTRDKIIQELVWHGSYYILRPLMMRNTQHIKYLVYRGFHSIPKKDIAQPKRRGNKVGDKHVWKQFWIGSQPNHSKNTLMDLWHSAFMKWCNDFPDEPTKYVAGWPNIHPMGLALCLHPPSPRREKWDKHMFTPDYDHMSSWPEFAAHMDDCWVHNTFQIPWFWPNHHGRFHAQNVFLVWFTTGIKAGLDLMNICMIWGDKPRTHKIIFHHMRDWKNHPIEFTMKMEEKYHWDGKFDYHLPFRYWDRMSIRHDNYKTSWPCGWPSWHYALIELTCYGEMEIYGWADQEYRQCDQTFHMQLQMQNMTKLIRWYTHLGCDSVMCVQSALHTLKEWKRPTRMIKNGLGLILNLMVMACAGHFTISDGFLLLESPWKWGNSYSGNSVEGILAVKGVDYDFDGDYAWEQRSNQQWVGGLMQCILAGLPRLNLEMFRQESMWANTYGMPSYKQTDFFHSNLIRTKRMDEMSHAMVRGWTGILVNFTHHNWTWYMSLLCQSAYGTARTIFTGGNPAADDNRDLDHEDDEVYDVWIDSECTAQWLSMPVGIPNYGFCTCQAKQWINRGIPAGKLFMEVCPWMNDNQKGASSHCYIIEWCS'\nt = predict(test_seq)\ndisplay(t)\ntest_seqs = ['YAWVHDHCNWCLERVGTVHDEKWTMEDPMVPKHCECNEPIQAWTQDNDLYNFCITLQICANNNRDGGNLIGRVDQLALQKRLLDQNWHKNCTPPSVVTRCNCATCEEKVETRVHFMKIMGESGWDGWVMYLPLGQQICYSIPHRQHKCWWSIWFVDFRLLPIGERNDTLSCFMIIKLEWIFDVCHLHDKSIEIAEAGVQIQVWAQQICSGTYIEGWLDWENSPIACPDGDYWSNWTAMVVAFCECRKRMCSVIPQTKFYAFHMVHNKWWLQWFYTSHEGKKIVGIEYHQGSCYYKWTKGEKHMHMNVEQRQWGADQVVHTPFNAWACWMHTAKHGDCNVPGQHWGMGWFRDDDELAAYGHHTEHGDATQLLFGTCYVCPLNIVYLMGTLVLPRHQHVRNKRPMVDRDMVYTRDKIIQELVWHGSYYILRPLMMRNTQHIKYLVYRGFHSIPKKDIAQPKRRGNKVGDKHVWKQFWIGSQPNHSKNTLMDLWHSAFMKWCNDFPDEPTKYVAGWPNIHPMGLALCLHPPSPRREKWDKHMFTPDYDHMSSWPEFAAHMDDCWVHNTFQIPWFWPNHHGRFHAQNVFLVWFTTGIKAGLDLMNICMIWGDKPRTHKIIFHHMRDWKNHPIEFTMKMEEKYHWDGKFDYHLPFRYWDRMSIRHDNYKTSWPCGWPSWHYALIELTCYGEMEIYGWADQEYRQCDQTFHMQLQMQNMTKLIRWYTHLGCDSVMCVQSALHTLKEWKRPTRMIKNGLGLILNLMVMACAGHFTISDGFLLLESPWKWGNSYSGNSVEGILAVKGVDYDFDGDYAWEQRSNQQWVGGLMQCILAGLPRLNLEMFRQESMWANTYGMPSYKQTDFFHSNLIRTKRMDEMSHAMVRGWTGILVNFTHHNWTWYMSLLCQSAYGTARTIFTGGNPAADDNRDLDHEDDEVYDVWIDSECTAQWLSMPVGIPNYGFCTCQAKQWINRGIPAGKLFMEVCPWMNDNQKGASSHCYIIEWCS','YAWVHDHCNVPKHCECNEPIQAWTQDNDLYNFCITLQICANNNRDGGNLIGRVDQLALQKRLLDQNWHKNCTPPSVVTRCNCATCEEKVETRVHFMKIMGESGWDGWVMYLPLGQQICYSIPHRQHKCWWSIWFVDFRLLPIGERNDTLSCFMIIKLEWIFDVCHLHDKSIEIAEAGVQIQVWAQQICSGTYIEGWLDWENSPIACPDGDYWSNWTAMVVAFCECRKRMCSVIPQTKFYAFHMVHNKWWLQWFYTSHEGKKIVGIEYHQGSCYYKWTKGEKHMHMNVEQRQWGADQVVHTPFNAWACWMHTAKHGDCNVPGQHWGMGWFRDDDELAAYGHHTEHGDATQLLFGTCYVCPLNIVYLMGTLVLPRHQHVRNKRPMVDRDMVYTRDKIIQELVWHGSYYILRPLMMRNTQHIKYLVYRGFHSIPKKDIAQPKRRGNKVGDKHVWKQFWIGSQPNHSKNTLMDLWHSAFMKWCNDFPDEPTKYVAGWPNIHPMGLALCLHPPSPRREKWDKHMFTPDYDHMSSWPEFAAHMDDCWVHNTFQIPWFWPNHHGRFHAQNVFLVWFTTGIKAGLDLMNICMIWGDKPRTHKIIFHHMRDWKNHPIEFTMKMEEKYHWDGKFDYHLPFRYWDRMSIRHDNYKTSWPCGWPSWHYALIELTCYGEMEIYGWADQEYRQCDQTFHMQLQMQNMTKLIRWYTHLGCDSVMCVQSALHTLKEWKRPTRMIKNGLGLILNLMVMACAGHFTISDGFLLLESPWKWGNSYSGNSVEGILAVKGVDYDFDGDYAWEQRSNQQWVGGLMQCILAGLPRLNLEMFRQESMWANTYGMPSYKQTDFFHSNLIRTKRMDEMSHAMVRGWTGILVNFTHHNWTWYMSLLCQSAYGTARTIFTGGNPAADDNRDLDHEDDEVYDVWIDSECTAQWLSMPVGIPNYGFCTCQAKQWINRGIPAGKLFMEVCPWMNDNQKGASSHCYIIEWCS']\nids,ts = predict_mult(['t1','t2'],test_seqs)\ndisplay(ts)\n#torch.nonzero(predict(test_seq))\n#labels_df.iloc[np.where(train_protein_ids=='P20536')[0][0]]","metadata":{"id":"t73eH9Tn1K4t","scrolled":true,"execution":{"iopub.status.busy":"2023-07-19T05:52:37.241286Z","iopub.status.idle":"2023-07-19T05:52:37.242369Z","shell.execute_reply.started":"2023-07-19T05:52:37.242119Z","shell.execute_reply":"2023-07-19T05:52:37.242142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.array(ts)","metadata":{"id":"7jqhlXTy1K4u","execution":{"iopub.status.busy":"2023-07-19T05:52:37.243628Z","iopub.status.idle":"2023-07-19T05:52:37.244758Z","shell.execute_reply.started":"2023-07-19T05:52:37.244503Z","shell.execute_reply":"2023-07-19T05:52:37.244525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t","metadata":{"id":"jpTtjiDO1K4v","execution":{"iopub.status.busy":"2023-07-19T05:52:37.245956Z","iopub.status.idle":"2023-07-19T05:52:37.247061Z","shell.execute_reply.started":"2023-07-19T05:52:37.246817Z","shell.execute_reply":"2023-07-19T05:52:37.246842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{"papermill":{"duration":0.021665,"end_time":"2023-05-09T08:41:01.780867","exception":false,"start_time":"2023-05-09T08:41:01.759202","status":"completed"},"tags":[],"id":"NsMJWTki1K4w"}},{"cell_type":"markdown","source":"For submission we will use the protein embeddings of the test data created by [Sergei Fironov](https://www.kaggle.com/sergeifironov) using the Rost Lab's T5 protein language model.","metadata":{"papermill":{"duration":0.02075,"end_time":"2023-05-09T08:41:01.82296","exception":false,"start_time":"2023-05-09T08:41:01.80221","status":"completed"},"tags":[],"id":"zRulLIuJ1K4x"}},{"cell_type":"code","source":"# Parse the fasta file\nkaggle_input_embeds = '/kaggle/input/t5embeds'\ntest_protein_ids = set(np.load(kaggle_input_embeds + '/test_ids.npy'))\ntest_records = SeqIO.parse(kaggle_input_data + '/Test (Targets)/testsuperset.fasta', format='fasta')\n\npred_ids, predictions = predict_mult(*list(zip(*[(record.id, str(record.seq)) for record in test_records if record.id in test_protein_ids[:100]])), verbose=True)","metadata":{"papermill":{"duration":10.290827,"end_time":"2023-05-09T08:41:12.134919","exception":false,"start_time":"2023-05-09T08:41:01.844092","status":"completed"},"tags":[],"id":"NnNy1w7z1K4x","scrolled":true,"execution":{"iopub.status.busy":"2023-07-19T05:52:37.248265Z","iopub.status.idle":"2023-07-19T05:52:37.249415Z","shell.execute_reply.started":"2023-07-19T05:52:37.249159Z","shell.execute_reply":"2023-07-19T05:52:37.249182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"papermill":{"duration":663.907351,"end_time":"2023-05-09T08:52:16.178461","exception":false,"start_time":"2023-05-09T08:41:12.27111","status":"completed"},"tags":[],"id":"QaYA0mN71K41","execution":{"iopub.status.busy":"2023-07-19T05:52:37.250603Z","iopub.status.idle":"2023-07-19T05:52:37.251507Z","shell.execute_reply.started":"2023-07-19T05:52:37.251259Z","shell.execute_reply":"2023-07-19T05:52:37.251286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the predictions we will create the submission data frame.\n\n**Note**: This will take atleast **15 to 20** minutes to finish.","metadata":{"id":"7EgfXYPY1K41"}},{"cell_type":"code","source":"# Reference: https://www.kaggle.com/code/alexandervc/baseline-multilabel-to-multitarget-binary\n\ndf_submission = pd.DataFrame(columns = ['Protein Id', 'GO Term Id','Prediction'])\nl = []\nfor k in list(test_protein_ids):\n    l += [ k] * predictions.shape[1]\n\ndf_submission['Protein Id'] = l\ndf_submission['GO Term Id'] = labels * predictions.shape[0]\ndf_submission['Prediction'] = predictions.ravel()\ndf_submission.to_csv(\"submission.tsv\",header=False, index=False, sep=\"\\t\")","metadata":{"id":"kPp6NFlC1K42","execution":{"iopub.status.busy":"2023-07-19T05:52:37.253118Z","iopub.status.idle":"2023-07-19T05:52:37.253590Z","shell.execute_reply.started":"2023-07-19T05:52:37.253345Z","shell.execute_reply":"2023-07-19T05:52:37.253368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission","metadata":{"papermill":{"duration":0.063739,"end_time":"2023-05-09T08:52:16.292974","exception":false,"start_time":"2023-05-09T08:52:16.229235","status":"completed"},"tags":[],"id":"-ZcOhlK61K42","execution":{"iopub.status.busy":"2023-07-19T05:52:37.255396Z","iopub.status.idle":"2023-07-19T05:52:37.255890Z","shell.execute_reply.started":"2023-07-19T05:52:37.255625Z","shell.execute_reply":"2023-07-19T05:52:37.255646Z"},"trusted":true},"execution_count":null,"outputs":[]}]}