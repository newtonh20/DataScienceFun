{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# CAFA 5 protein function Prediction with TensorFlow\n\nThis notebook walks you through how to train a DNN model using TensorFlow on the CAFA 5 protein function Prediction dataset made available for this competition.\n\nThe objective of the model is to predict the function(aka **GO term ID**) of a set of proteins based on their amino acid sequences and other data.\n\n\n**Note** : This notebook runs without any GPU. This is because enabling GPUs leaves less RAM memory on the VM and the submission step needs a lot of memory. One point where this would impact is when training the model. With CPU it will take around 2 minutes while on GPU it would take around 30 seconds.","metadata":{"papermill":{"duration":0.008875,"end_time":"2023-05-09T08:30:09.052999","exception":false,"start_time":"2023-05-09T08:30:09.044124","status":"completed"},"tags":[],"id":"pb-4zvGq1K4C"}},{"cell_type":"markdown","source":"## About the Data\n\n### Protein Sequence\n\nEach protein is composed of dozens or hundreds of amino acids that are linked sequentially. Each amino acid in the sequence may be represented by a one-letter or three-letter code. Thus the sequence of a protein is often notated as a string of letters.\n\n<img src=\"https://cityu-bioinformatics.netlify.app/img/tools/protein/pro_seq.png\" alt =\"Sequence.png\" style='width: 800px;' >\n\nImage source - [https://cityu-bioinformatics.netlify.app/](https://cityu-bioinformatics.netlify.app/too2/new_proteo/pro_seq/)\n\nThe `train_sequences.fasta` made available for this competitions, contains the sequences for proteins with annotations (labelled proteins).","metadata":{"papermill":{"duration":0.009266,"end_time":"2023-05-09T08:30:09.071132","exception":false,"start_time":"2023-05-09T08:30:09.061866","status":"completed"},"tags":[],"id":"B8H6QR-M1K4J"}},{"cell_type":"markdown","source":"# Gene Ontology\n\nWe can define the functional properties of a proteins using Gene Ontology(GO). Gene Ontology (GO) describes our understanding of the biological domain with respect to three aspects:\n1. Molecular Function (MF)\n2. Biological Process (BP)\n3. Cellular Component (CC)\n\nRead more about Gene Ontology [here](http://geneontology.org/docs/ontology-documentation).\n\nFile `train_terms.tsv` contains the list of annotated terms (ground truth) for the proteins in `train_sequences.fasta`. In `train_terms.tsv` the first column indicates the protein's UniProt accession ID (unique protein id), the second is the `GO Term ID`, and the third indicates in which ontology the term appears.","metadata":{"papermill":{"duration":0.008355,"end_time":"2023-05-09T08:30:09.08863","exception":false,"start_time":"2023-05-09T08:30:09.080275","status":"completed"},"tags":[],"id":"vYXassg_1K4L"}},{"cell_type":"markdown","source":"# Labels of the dataset\n\nThe objective of our model is to predict the terms (functions) of a protein sequence. One protein sequence can have many functions and can thus be classified into any number of terms. Each term is uniquely identified by a `GO Term ID`. Thus our model has to predict all the `GO Term ID`s for a protein sequence. This means that the task at hand is a multi-label classification problem.","metadata":{"id":"U1wpCeJH1K4M"}},{"cell_type":"markdown","source":"\n","metadata":{"papermill":{"duration":0.008308,"end_time":"2023-05-09T08:30:09.105539","exception":false,"start_time":"2023-05-09T08:30:09.097231","status":"completed"},"tags":[],"id":"M042PgfH1K4N"}},{"cell_type":"markdown","source":"# Protein embeddings for train and test data\n\nTo train a machine learning model we cannot use the alphabetical protein sequences in`train_sequences.fasta` directly. They have to be converted into a vector format. In this notebook, we will use embeddings of the protein sequences to train the model. You can think of protein embeddings to be similar to word embeddings used to train NLP models.\n<!-- Instead, to make calculations and data preparation easier we will use precalculated protein embeddings.\n -->\nProtein embeddings are a machine-friendly method of capturing the protein's structural and functional characteristics, mainly through its sequence. One approach is to train a custom ML model to learn the protein embeddings of the protein sequences in the dataset being used in this notebook. Since this dataset represents proteins using amino-acid sequences which is a standard approach, we can use any publicly available pre-trained protein embedding models to generate the embeddings.\n\nThere are a variety of protein embedding models. To make data preparation easier, we have used the precalculated protein embeddings created by [Sergei Fironov](https://www.kaggle.com/sergeifironov) using the Rost Lab's T5 protein language model in this notebook. The precalculated protein embeddings can be found [here](https://www.kaggle.com/datasets/sergeifironov/t5embeds). We have added this dataset to the notebook along with the dataset made available for the competition.\n\nTo add this to your enviroment, on the right side panel, click on `Add Data` and search for `t5embeds` (make sure that it's the correct [one](https://www.kaggle.com/datasets/sergeifironov/t5embeds)) and then click on the `+` beside it.\n\n**We'll start by introducing a variable that decides whether we prepare the training data cell by cell or if we do it all at once (for memory issues)**\n","metadata":{"papermill":{"duration":0.008548,"end_time":"2023-05-09T08:30:09.122603","exception":false,"start_time":"2023-05-09T08:30:09.114055","status":"completed"},"tags":[],"id":"tD6exQ-11K4N"}},{"cell_type":"code","source":"notebook_run = False","metadata":{"execution":{"iopub.status.busy":"2023-07-20T01:31:11.096591Z","iopub.execute_input":"2023-07-20T01:31:11.097241Z","iopub.status.idle":"2023-07-20T01:31:11.102408Z","shell.execute_reply.started":"2023-07-20T01:31:11.097204Z","shell.execute_reply":"2023-07-20T01:31:11.101344Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Import the Required Libraries","metadata":{"papermill":{"duration":0.009086,"end_time":"2023-05-09T08:30:09.140473","exception":false,"start_time":"2023-05-09T08:30:09.131387","status":"completed"},"tags":[],"id":"HeXcmUEb1K4O"}},{"cell_type":"code","source":"import torch\ntorch.version.cuda","metadata":{"execution":{"iopub.status.busy":"2023-07-20T01:31:11.117418Z","iopub.execute_input":"2023-07-20T01:31:11.117935Z","iopub.status.idle":"2023-07-20T01:31:11.124812Z","shell.execute_reply.started":"2023-07-20T01:31:11.117901Z","shell.execute_reply":"2023-07-20T01:31:11.123794Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'11.8'"},"metadata":{}}]},{"cell_type":"code","source":"# Install required packages.\nimport os\nimport torch\nos.environ['TORCH'] = torch.__version__\nos.environ['CUDA'] = torch.version.cuda\nprint(torch.__version__)\nprint(torch.version.cuda)\n\n!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.0%2Bcu118.html\n!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0%2Bcu118.html\n!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n!pip install torchmetrics\n!pip install networkx Bio","metadata":{"id":"9oq-a6oG1K4P","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc85caf6-e48e-4031-ee0a-9e74da3cfab6","execution":{"iopub.status.busy":"2023-07-20T01:31:11.128852Z","iopub.execute_input":"2023-07-20T01:31:11.129177Z","iopub.status.idle":"2023-07-20T01:32:28.469796Z","shell.execute_reply.started":"2023-07-20T01:31:11.129153Z","shell.execute_reply":"2023-07-20T01:32:28.468579Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"2.0.0\n11.8\nLooking in links: https://data.pyg.org/whl/torch-2.0.0%2Bcu118.html\nCollecting torch-scatter\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.1%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-scatter\nSuccessfully installed torch-scatter-2.1.1+pt20cu118\nLooking in links: https://data.pyg.org/whl/torch-2.0.0%2Bcu118.html\nCollecting torch-sparse\n  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.17%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-sparse) (1.11.1)\nRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from scipy->torch-sparse) (1.23.5)\nInstalling collected packages: torch-sparse\nSuccessfully installed torch-sparse-0.6.17+pt20cu118\nCollecting git+https://github.com/pyg-team/pytorch_geometric.git\n  Cloning https://github.com/pyg-team/pytorch_geometric.git to /tmp/pip-req-build-022ad93p\n  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /tmp/pip-req-build-022ad93p\n  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 87ca71913efa164efe07622b5e1fcd189125d39c\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (4.65.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (1.23.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (1.11.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (3.1.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (2.31.0)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (3.0.9)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric==2.4.0) (5.9.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric==2.4.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric==2.4.0) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric==2.4.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric==2.4.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric==2.4.0) (2023.5.7)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric==2.4.0) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric==2.4.0) (3.1.0)\nBuilding wheels for collected packages: torch_geometric\n  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for torch_geometric: filename=torch_geometric-2.4.0-py3-none-any.whl size=971733 sha256=b9691a5991acbcf32e631fd60de595f2cc576c4b9afb5c1ef609917f5acea6fa\n  Stored in directory: /tmp/pip-ephem-wheel-cache-nwfidogu/wheels/d3/78/eb/9e26525b948d19533f1688fb6c209cec8a0ba793d39b49ae8f\nSuccessfully built torch_geometric\nInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.4.0\nRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.0.0)\nRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.23.5)\nRequirement already satisfied: torch>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.0.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: lightning-utilities>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.9.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.7.0->torchmetrics) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->torchmetrics) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (3.1)\nCollecting Bio\n  Downloading bio-1.5.9-py3-none-any.whl (276 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.4/276.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: biopython>=1.80 in /opt/conda/lib/python3.10/site-packages (from Bio) (1.81)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from Bio) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from Bio) (4.65.0)\nCollecting mygene (from Bio)\n  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from Bio) (1.5.3)\nRequirement already satisfied: pooch in /opt/conda/lib/python3.10/site-packages (from Bio) (1.6.0)\nCollecting gprofiler-official (from Bio)\n  Downloading gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from biopython>=1.80->Bio) (1.23.5)\nCollecting biothings-client>=0.2.6 (from mygene->Bio)\n  Downloading biothings_client-0.3.0-py2.py3-none-any.whl (29 kB)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->Bio) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->Bio) (2023.3)\nRequirement already satisfied: appdirs>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from pooch->Bio) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pooch->Bio) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->Bio) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->Bio) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->Bio) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->Bio) (2023.5.7)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pooch->Bio) (3.0.9)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->Bio) (1.16.0)\nInstalling collected packages: gprofiler-official, biothings-client, mygene, Bio\nSuccessfully installed Bio-1.5.9 biothings-client-0.3.0 gprofiler-official-1.0.0 mygene-3.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom scipy.sparse import coo_matrix\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GINConv, global_add_pool, GCNConv, global_max_pool, global_mean_pool, GATConv, SAGEConv\nfrom torch_geometric.data import Data, Batch\nfrom torch_geometric.loader.dataloader import DataLoader\nfrom torch.utils.data import Dataset\nfrom torchmetrics import Accuracy\n\nfrom torch_geometric.utils import from_networkx, to_networkx, to_dense_batch, convert\nfrom torch.nn import Sequential, Linear, BatchNorm1d, ReLU, SELU\nfrom torch_geometric.nn.models import GAT\nfrom sklearn.model_selection import train_test_split\nimport torch.optim as optim\nimport torch_geometric.nn.models as models\nfrom Bio import SeqIO\nfrom sklearn.metrics import classification_report\nimport concurrent.futures\n\n# Required for progressbar widget\nimport progressbar\nfrom datetime import datetime\nimport random\nrandom.seed(datetime.now().timestamp())","metadata":{"papermill":{"duration":9.85331,"end_time":"2023-05-09T08:30:19.002985","exception":false,"start_time":"2023-05-09T08:30:09.149675","status":"completed"},"tags":[],"id":"nb0xJgIp1K4R","execution":{"iopub.status.busy":"2023-07-20T01:32:28.473138Z","iopub.execute_input":"2023-07-20T01:32:28.473525Z","iopub.status.idle":"2023-07-20T01:32:38.852379Z","shell.execute_reply.started":"2023-07-20T01:32:28.473492Z","shell.execute_reply":"2023-07-20T01:32:38.851386Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:39: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n/opt/conda/lib/python3.10/site-packages/torch_geometric/typing.py:70: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /opt/conda/lib/python3.10/site-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Numpy v\" + np.__version__)\nkaggle_input_data = '/kaggle/input/cafa-5-protein-function-prediction'","metadata":{"papermill":{"duration":0.018272,"end_time":"2023-05-09T08:30:19.030432","exception":false,"start_time":"2023-05-09T08:30:19.01216","status":"completed"},"tags":[],"id":"x6XxiN4L1K4S","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b22d5bd0-a35c-4def-80e2-484c17d35a4b","execution":{"iopub.status.busy":"2023-07-20T01:32:38.854095Z","iopub.execute_input":"2023-07-20T01:32:38.854458Z","iopub.status.idle":"2023-07-20T01:32:38.861729Z","shell.execute_reply.started":"2023-07-20T01:32:38.854424Z","shell.execute_reply":"2023-07-20T01:32:38.860707Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Numpy v1.23.5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load the Dataset","metadata":{"papermill":{"duration":0.008429,"end_time":"2023-05-09T08:30:19.047756","exception":false,"start_time":"2023-05-09T08:30:19.039327","status":"completed"},"tags":[],"id":"WU7qEirr1K4T"}},{"cell_type":"markdown","source":"First we will load the file `train_terms.tsv` which contains the list of annotated terms (functions) for the proteins. We will extract the labels aka `GO term ID` and create a label dataframe for the protein embeddings.","metadata":{"papermill":{"duration":0.008388,"end_time":"2023-05-09T08:30:19.065367","exception":false,"start_time":"2023-05-09T08:30:19.056979","status":"completed"},"tags":[],"id":"Fn7EjFw01K4T"}},{"cell_type":"code","source":"if notebook_run:\n    #train_terms = pd.read_csv(\"/kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv\",sep=\"\\t\")\n    train_terms = pd.read_csv(kaggle_input_data + \"/Train/train_terms.tsv\",sep=\"\\t\")\n    print(train_terms.shape)","metadata":{"papermill":{"duration":3.69155,"end_time":"2023-05-09T08:30:22.766144","exception":false,"start_time":"2023-05-09T08:30:19.074594","status":"completed"},"tags":[],"id":"ggKV4PCs1K4T","colab":{"base_uri":"https://localhost:8080/"},"outputId":"25f031ff-bab8-4cc5-ca0d-21e8d38de8cd","execution":{"iopub.status.busy":"2023-07-20T01:32:38.864972Z","iopub.execute_input":"2023-07-20T01:32:38.867379Z","iopub.status.idle":"2023-07-20T01:32:38.895712Z","shell.execute_reply.started":"2023-07-20T01:32:38.867341Z","shell.execute_reply":"2023-07-20T01:32:38.894810Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"`train_terms` dataframe is composed of 3 columns and 5363863 entries. We can see all 3 dimensions of our dataset by printing out the first 5 entries using the following code:","metadata":{"papermill":{"duration":0.008358,"end_time":"2023-05-09T08:30:22.783293","exception":false,"start_time":"2023-05-09T08:30:22.774935","status":"completed"},"tags":[],"id":"Vr98Z1qX1K4T"}},{"cell_type":"code","source":"if notebook_run:\n    train_terms = train_terms.sample(150000)","metadata":{"papermill":{"duration":0.038607,"end_time":"2023-05-09T08:30:22.830633","exception":false,"start_time":"2023-05-09T08:30:22.792026","status":"completed"},"tags":[],"id":"Cvb-XyY91K4U","execution":{"iopub.status.busy":"2023-07-20T01:32:38.898006Z","iopub.execute_input":"2023-07-20T01:32:38.898437Z","iopub.status.idle":"2023-07-20T01:32:38.917937Z","shell.execute_reply.started":"2023-07-20T01:32:38.898398Z","shell.execute_reply":"2023-07-20T01:32:38.915479Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"if notebook_run:\n    train_terms.loc[train_terms['EntryID'] == 'Q59VP0']","metadata":{"scrolled":true,"id":"brMGEGHw1K4U","colab":{"base_uri":"https://localhost:8080/","height":49},"outputId":"4832995c-95d7-4f41-f391-2935aaf1ee76","execution":{"iopub.status.busy":"2023-07-20T01:32:38.919710Z","iopub.execute_input":"2023-07-20T01:32:38.920375Z","iopub.status.idle":"2023-07-20T01:32:38.936902Z","shell.execute_reply.started":"2023-07-20T01:32:38.920338Z","shell.execute_reply":"2023-07-20T01:32:38.935483Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"If we look at the first entry of `train_terms.tsv`, we can see that it contains protein id(`A0A009IHW8`), the GO term(`GO:0008152`) and its aspect(`BPO`).","metadata":{"papermill":{"duration":0.008764,"end_time":"2023-05-09T08:30:22.848867","exception":false,"start_time":"2023-05-09T08:30:22.840103","status":"completed"},"tags":[],"id":"HsxDFooS1K4U"}},{"cell_type":"markdown","source":"# A different kind of embedding\n\nWe introduce a new embedding for Proteins based on the notion of Recurrence Networks: Olyaee, M. H., Yaghoubi, A., & Yaghoobi, M. (2016). Predicting protein structural classes based on complex networks and recurrence analysis. Journal of theoretical biology, 404, 375–382. https://doi.org/10.1016/j.jtbi.2016.06.018\n\nInstead of using the provided embeddings, we will train Graph Neural Networks on a new embedding for each graph, defined by recurrence networks.\n\n**Let's start by defining a function that takes a Protein Sequence String and creates a recurrence network:**","metadata":{"id":"MbqodfXE1K4W"}},{"cell_type":"code","source":"from Bio.SeqUtils.ProtParamData import *\nfrom collections import Counter\n\ndef protein_recurrence_network(sequence):\n    # Create an empty graph for the recurrence network\n    graph = nx.DiGraph()\n\n    # Use a Counter to store the edge weights\n    edge_weights = Counter(zip(sequence, sequence[1:]))\n\n    # Add nodes and edges to the graph based on the edge weights using list comprehension\n    graph.add_weighted_edges_from(((char1, char2, weight / len(sequence)) \n                                    for (char1, char2), weight in edge_weights.items()))\n    return protein_attributes(graph)\n\n\ndef protein_attributes(graph):\n    nx.set_node_attributes(graph, nx.betweenness_centrality(graph), \"betweenness\")\n\n     # Create a dictionary of attribute values for all nodes\n    attributes = {}\n    for node in graph.nodes:\n        att_dict = {\n            k: v[node] if node in v else 0. for k, v in gravy_scales.items()\n        }\n        att_dict.update({\n            'flexibility': Flex.get(node, 0.),\n            'hydrophilicity': hw.get(node, 0.),\n            'surface_accessibility': em.get(node, 0.),\n            'janin_interior': ja.get(node, 0.)\n        })\n        attributes[node] = att_dict\n\n    # Update attributes for all nodes in a single step\n    nx.set_node_attributes(graph, attributes)\n\n    # Now setup interaction details\n    for v1,v2 in graph.edges:\n        try:\n            graph[v1][v2]['instability_index'] = DIWV[v1][v2]\n        except KeyError:\n            graph[v1][v2]['instability_index'] = 0.\n\n    return graph","metadata":{"id":"FI3rmj3H1K4X","execution":{"iopub.status.busy":"2023-07-20T01:32:38.940322Z","iopub.execute_input":"2023-07-20T01:32:38.942700Z","iopub.status.idle":"2023-07-20T01:32:38.969245Z","shell.execute_reply.started":"2023-07-20T01:32:38.942635Z","shell.execute_reply":"2023-07-20T01:32:38.968021Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Define a function to process a record and return the protein recurrence network\n#def process_record(record):\n#    return record.id, protein_recurrence_network(str(record.seq))\ndef process_record(record):\n    if record.id in train_terms_set:\n        return record.id, protein_recurrence_network(str(record.seq))\n    else:\n        return None\n\nif notebook_run:\n    train_terms_set = set(train_terms['EntryID'].values)\n    \n    # Parse the fasta file\n    records = SeqIO.parse(kaggle_input_data + \"/Train/train_sequences.fasta\", \"fasta\")\n\n    # Use map to process the records in parallel\n    protein_rn_dict = dict(map(process_record, (record for record in records if record.id in train_terms['EntryID'].values)))\n    g_train_protein_ids = np.array(list(protein_rn_dict.keys()))\n    \n    # Use concurrent.futures to process the records in parallel\n    #with concurrent.futures.ProcessPoolExecutor() as executor:\n    #    future_to_record = {executor.submit(process_record, record): record for record in records}\n    #    protein_rn_dict = {future.result()[0]: future.result()[1] for future in concurrent.futures.as_completed(future_to_record) if future.result() is not None}\n    #    g_train_protein_ids = np.array(list(protein_rn_dict.keys()))","metadata":{"id":"VD5T4vMS1K4X","execution":{"iopub.status.busy":"2023-07-20T01:32:38.970835Z","iopub.execute_input":"2023-07-20T01:32:38.971548Z","iopub.status.idle":"2023-07-20T01:32:38.986133Z","shell.execute_reply.started":"2023-07-20T01:32:38.971510Z","shell.execute_reply":"2023-07-20T01:32:38.985278Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Let's draw one of these graphs to see what it looks like.","metadata":{"id":"bD5sC7v01K4X"}},{"cell_type":"code","source":"if notebook_run:\n    G_id = train_terms['EntryID'].sample(1).values[0]\n    G = protein_rn_dict[G_id]\n    fig = plt.figure(figsize=(12,12))\n    # Extract edge weights\n    edge_weights = nx.get_edge_attributes(G, 'weight')\n    # Round the edge weights to 4 decimal places\n    edge_weights = {k: round(v, 4) for k, v in edge_weights.items()}\n\n    # Draw the graph\n    pos = nx.spring_layout(G)\n    nx.draw_networkx(G, pos, with_labels=True, node_color='lightblue')\n\n    # Draw edge labels\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_weights, font_color='red', font_size=8)\n    plt.title(G_id)\n\n    # Show the graph\n    plt.axis('off')\n    plt.show()","metadata":{"id":"BxPwR0if1K4X","colab":{"base_uri":"https://localhost:8080/","height":961},"outputId":"ae50251b-462b-4d18-f046-0816358ea0ca","execution":{"iopub.status.busy":"2023-07-20T01:32:38.987470Z","iopub.execute_input":"2023-07-20T01:32:38.988333Z","iopub.status.idle":"2023-07-20T01:32:39.000586Z","shell.execute_reply.started":"2023-07-20T01:32:38.988284Z","shell.execute_reply":"2023-07-20T01:32:38.999763Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"if notebook_run:\n    list(G.nodes(data=True))\n    #for v1,v2 in G.edges:\n    #  print(v1)\n    #  print(v2)","metadata":{"id":"Km07L70J4YFG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"014b1315-56af-4392-833b-7880a77cb349","scrolled":true,"execution":{"iopub.status.busy":"2023-07-20T01:32:39.005187Z","iopub.execute_input":"2023-07-20T01:32:39.006101Z","iopub.status.idle":"2023-07-20T01:32:39.011799Z","shell.execute_reply.started":"2023-07-20T01:32:39.006064Z","shell.execute_reply":"2023-07-20T01:32:39.010961Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"if notebook_run:\n    # Just looking for weird proteins\n    for k in protein_rn_dict:\n        if 'U' in protein_rn_dict[k].nodes():\n            print(k)\n            print(protein_rn_dict[k].nodes())\n            print(protein_rn_dict[k])","metadata":{"scrolled":true,"id":"f4XokvA01K4X","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f36baad6-6ce8-4669-ab92-263eb903cec6","execution":{"iopub.status.busy":"2023-07-20T01:32:39.013228Z","iopub.execute_input":"2023-07-20T01:32:39.013874Z","iopub.status.idle":"2023-07-20T01:32:39.022895Z","shell.execute_reply.started":"2023-07-20T01:32:39.013838Z","shell.execute_reply":"2023-07-20T01:32:39.022213Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Prepare the dataset\n\nReference: https://www.kaggle.com/code/alexandervc/baseline-multilabel-to-multitarget-binary","metadata":{"papermill":{"duration":0.009208,"end_time":"2023-05-09T08:30:32.825978","exception":false,"start_time":"2023-05-09T08:30:32.81677","status":"completed"},"tags":[],"id":"EJ6434191K4X"}},{"cell_type":"markdown","source":"First we will extract all the needed labels(`GO term ID`) from `train_terms.tsv` file. There are more than 40,000 labels. In order to simplify our model, we will choose the most frequent 1500 `GO term ID`s as labels.","metadata":{"papermill":{"duration":0.009674,"end_time":"2023-05-09T08:30:32.845065","exception":false,"start_time":"2023-05-09T08:30:32.835391","status":"completed"},"tags":[],"id":"6LjXf3Bp1K4X"}},{"cell_type":"markdown","source":"Let's plot the most frequent 100 `GO Term ID`s in `train_terms.tsv`.","metadata":{"papermill":{"duration":0.009238,"end_time":"2023-05-09T08:30:32.863785","exception":false,"start_time":"2023-05-09T08:30:32.854547","status":"completed"},"tags":[],"id":"PasOnoXy1K4Y"}},{"cell_type":"code","source":"if notebook_run:\n    # Select first 1500 values for plotting\n    plot_df = train_terms['term'].value_counts().iloc[:100]\n\n    figure, axis = plt.subplots(1, 1, figsize=(12, 6))\n\n    bp = sns.barplot(ax=axis, x=np.array(plot_df.index), y=plot_df.values)\n    bp.set_xticklabels(bp.get_xticklabels(), rotation=90, size = 6)\n    axis.set_title('Top 100 frequent GO term IDs')\n    bp.set_xlabel(\"GO term IDs\", fontsize = 12)\n    bp.set_ylabel(\"Count\", fontsize = 12)\n    plt.show()","metadata":{"papermill":{"duration":1.592489,"end_time":"2023-05-09T08:30:34.465912","exception":false,"start_time":"2023-05-09T08:30:32.873423","status":"completed"},"tags":[],"id":"23FUlWzw1K4Y","colab":{"base_uri":"https://localhost:8080/","height":607},"outputId":"6fb28ae9-631c-4286-b831-7987248df807","execution":{"iopub.status.busy":"2023-07-20T01:32:39.024360Z","iopub.execute_input":"2023-07-20T01:32:39.024986Z","iopub.status.idle":"2023-07-20T01:32:39.037018Z","shell.execute_reply.started":"2023-07-20T01:32:39.024951Z","shell.execute_reply":"2023-07-20T01:32:39.036154Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"We will now save the first 1500 most frequent GO term Ids into a list.","metadata":{"papermill":{"duration":0.010458,"end_time":"2023-05-09T08:30:34.487707","exception":false,"start_time":"2023-05-09T08:30:34.477249","status":"completed"},"tags":[],"id":"UbjLGFEK1K4Y"}},{"cell_type":"code","source":"if notebook_run:\n    # Set the limit for label\n    num_of_labels = 1500\n\n    # Take value counts in descending order and fetch first 1500 `GO term ID` as labels\n    labels = train_terms['term'].value_counts().index[:num_of_labels].tolist()","metadata":{"papermill":{"duration":0.523976,"end_time":"2023-05-09T08:30:35.021974","exception":false,"start_time":"2023-05-09T08:30:34.497998","status":"completed"},"tags":[],"id":"PePJVY871K4Y","execution":{"iopub.status.busy":"2023-07-20T01:32:39.040543Z","iopub.execute_input":"2023-07-20T01:32:39.040852Z","iopub.status.idle":"2023-07-20T01:32:39.047514Z","shell.execute_reply.started":"2023-07-20T01:32:39.040826Z","shell.execute_reply":"2023-07-20T01:32:39.046464Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Next, we will create a new dataframe by filtering the train terms with the selected `GO Term ID`s.","metadata":{"papermill":{"duration":0.009833,"end_time":"2023-05-09T08:30:35.042088","exception":false,"start_time":"2023-05-09T08:30:35.032255","status":"completed"},"tags":[],"id":"rfoR7jaK1K4f"}},{"cell_type":"code","source":"if notebook_run:\n    # Fetch the train_terms data for the relevant labels only\n    train_terms_updated = train_terms.loc[train_terms['term'].isin(labels)]","metadata":{"papermill":{"duration":0.668657,"end_time":"2023-05-09T08:30:35.720953","exception":false,"start_time":"2023-05-09T08:30:35.052296","status":"completed"},"tags":[],"id":"QmbKQ-wX1K4g","execution":{"iopub.status.busy":"2023-07-20T01:32:39.049671Z","iopub.execute_input":"2023-07-20T01:32:39.050600Z","iopub.status.idle":"2023-07-20T01:32:39.058579Z","shell.execute_reply.started":"2023-07-20T01:32:39.050563Z","shell.execute_reply":"2023-07-20T01:32:39.057544Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Let us plot the aspect values in the new **train_terms_updated** dataframe using a pie chart.","metadata":{"papermill":{"duration":0.009797,"end_time":"2023-05-09T08:30:35.741328","exception":false,"start_time":"2023-05-09T08:30:35.731531","status":"completed"},"tags":[],"id":"hs4Ndc0k1K4g"}},{"cell_type":"code","source":"if notebook_run:\n    pie_df = train_terms_updated['aspect'].value_counts()\n    palette_color = sns.color_palette('bright')\n    plt.pie(pie_df.values, labels=np.array(pie_df.index), colors=palette_color, autopct='%.0f%%')\n    plt.show()","metadata":{"papermill":{"duration":0.419624,"end_time":"2023-05-09T08:30:36.171193","exception":false,"start_time":"2023-05-09T08:30:35.751569","status":"completed"},"tags":[],"id":"F3UvKHwt1K4g","colab":{"base_uri":"https://localhost:8080/","height":406},"outputId":"702cad97-8748-4bd7-efa5-74bc3f17486c","execution":{"iopub.status.busy":"2023-07-20T01:32:39.061717Z","iopub.execute_input":"2023-07-20T01:32:39.062563Z","iopub.status.idle":"2023-07-20T01:32:39.069304Z","shell.execute_reply.started":"2023-07-20T01:32:39.062526Z","shell.execute_reply":"2023-07-20T01:32:39.068327Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"As you can see, majority of the `GO term Id`s have BPO(Biological Process Ontology) as their aspect.","metadata":{"papermill":{"duration":0.016642,"end_time":"2023-05-09T08:30:36.204943","exception":false,"start_time":"2023-05-09T08:30:36.188301","status":"completed"},"tags":[],"id":"AKV3JhgU1K4h"}},{"cell_type":"markdown","source":"Since this is a multi label classification problem, in the labels array we will denote the presence or absence of each Go Term Id for a protein id using a 1 or 0.\nFirst, we will create a numpy array `train_labels` of required size for the labels. To update the `train_labels` array with the appropriate values, we will loop through the label list.","metadata":{"id":"0sexUW1v1K4i"}},{"cell_type":"code","source":"if notebook_run:\n    # Setup progressbar settings.\n    # This is strictly for aesthetic.\n    bar = progressbar.ProgressBar(maxval=num_of_labels, \\\n        widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n\n    # Create an empty dataframe of required size for storing the labels,\n    # i.e, train_size x num_of_labels (142246 x 1500)\n    train_size = g_train_protein_ids.shape[0] # len(X)\n    train_labels = np.zeros((train_size, num_of_labels))\n\n    # Convert from numpy to pandas series for better handling\n    series_train_protein_ids = pd.Series(g_train_protein_ids)\n\n    # Generate a dict where key is label and value is the corresponding proteins\n    def get_label_proteins(label):\n        proteins = train_terms_updated[train_terms_updated['term'] == label]['EntryID'].unique()\n        return label, proteins\n\n    label_protein_dict = dict(map(get_label_proteins, labels))\n\n    # Loop through each label\n    for i, label in enumerate(labels):\n        # Fetch all the unique EntryId aka proteins related to the current label(GO term ID)\n        label_related_proteins = label_protein_dict[label]\n\n        # In the series_train_protein_ids pandas series, if a protein is related\n        # to the current label, then mark it as 1, else 0.\n        # Replace the ith column of train_labels with that pandas series.\n        train_labels[:,i] =  series_train_protein_ids.isin(label_related_proteins).astype(float)\n\n        # Progress bar percentage increase\n        bar.update(i+1)\n\n    # Notify the end of progress bar\n    bar.finish()\n\n    # Convert train_labels numpy into pandas dataframe\n    labels_df = pd.DataFrame(data = train_labels, columns = labels)\n    print(labels_df.shape)\n","metadata":{"id":"qM8RdXsyDoDQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"27e9aff0-3c75-4976-8c59-221ef156dbaf","execution":{"iopub.status.busy":"2023-07-20T01:32:39.072644Z","iopub.execute_input":"2023-07-20T01:32:39.072983Z","iopub.status.idle":"2023-07-20T01:32:39.082839Z","shell.execute_reply.started":"2023-07-20T01:32:39.072958Z","shell.execute_reply":"2023-07-20T01:32:39.081688Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"The final labels dataframe (`label_df`) is composed of 1500 columns and 142246 entries (though we brought it down to 10000). We can see all 1500 dimensions(results will be truncated since the number of columns is big) of our dataset by printing out the first 5 entries using the following code:","metadata":{"papermill":{"duration":0.010097,"end_time":"2023-05-09T08:38:51.971947","exception":false,"start_time":"2023-05-09T08:38:51.96185","status":"completed"},"tags":[],"id":"nrQKV4Cn1K4j"}},{"cell_type":"code","source":"if notebook_run:\n    labels_df.head()","metadata":{"papermill":{"duration":0.048128,"end_time":"2023-05-09T08:38:52.031041","exception":false,"start_time":"2023-05-09T08:38:51.982913","status":"completed"},"tags":[],"id":"KZeiIuqY1K4k","colab":{"base_uri":"https://localhost:8080/","height":299},"outputId":"515c6879-e3d7-4a6b-cef6-68f20eeae8e0","execution":{"iopub.status.busy":"2023-07-20T01:32:39.084502Z","iopub.execute_input":"2023-07-20T01:32:39.085218Z","iopub.status.idle":"2023-07-20T01:32:39.096538Z","shell.execute_reply.started":"2023-07-20T01:32:39.085184Z","shell.execute_reply":"2023-07-20T01:32:39.095767Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"if notebook_run:\n    num_of_labels","metadata":{"id":"bIl4xgNi1K4k","colab":{"base_uri":"https://localhost:8080/"},"outputId":"98c17290-242a-4782-e829-f33f7475870e","execution":{"iopub.status.busy":"2023-07-20T01:32:39.098587Z","iopub.execute_input":"2023-07-20T01:32:39.098865Z","iopub.status.idle":"2023-07-20T01:32:39.106640Z","shell.execute_reply.started":"2023-07-20T01:32:39.098840Z","shell.execute_reply":"2023-07-20T01:32:39.105682Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Graph Preparation\n\nNext, we'll define our GNN model class, which will inherit from nn.Module:","metadata":{"id":"oIKSnaYQ1K4l"}},{"cell_type":"code","source":"import torch_geometric\nclass GraphSAGE(torch.nn.Module):\n    def __init__(self, in_channels=1, hidden_dim = 128, out_channels=1500):\n        \"\"\"\n        Represents a 3-layer GraphSAGE GNN model\n        with embedding and hidden dimension of hidden_dim.\n        \"\"\"\n        super(GraphSAGE, self).__init__()\n        self.tag = 'GraphSAGE'\n        self.pre_embs = nn.Embedding(in_channels, hidden_dim)\n\n        self.convs = nn.ModuleList()\n        self.selus = nn.ModuleList()\n        self.dropouts = nn.ModuleList()\n        self.layer_norms = nn.ModuleList()\n\n        # Input layer\n        self.convs.append(SAGEConv(in_channels, hidden_dim))\n        self.selus.append(nn.SELU())\n        self.dropouts.append(nn.Dropout(0.1))\n        self.layer_norms.append(nn.LayerNorm(hidden_dim))\n\n        # Hidden layers\n        for _ in range(3):\n            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n            self.selus.append(SELU())\n            self.dropouts.append(nn.Dropout(0.1))\n            self.layer_norms.append(nn.LayerNorm(hidden_dim))\n\n        # Output layer\n        self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n        self.out = nn.Linear(3*hidden_dim, out_channels)\n        self.out.weight.data.fill_(1.)\n        self.dropout = nn.Dropout(0.1)\n            \n    def forward(self, data):\n        \"\"\"\n        Runs a forward pass through GraphSAGE with given initial skill IDs and\n        edge_index and edge_weights.\n\n        Arguments:\n          - x: amino acid IDs (torch.Tensor)\n          - edge_index: edges in skill graph (torch.Tensor)\n          - edge_weight: edge weights of skill graph (torch.Tensor)\n\n        Returns:\n          - final node embedding for skill\n        \"\"\"\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        # Convert x tensor to Long type\n        #x = x.to(torch.long)\n        #x = self.pre_embs(x)\n        for conv, selu, dropout, layer_norm in zip(self.convs, self.selus, self.dropouts, self.layer_norms):\n          x = layer_norm(dropout(selu(conv(x, edge_index))))\n\n        x_add = global_add_pool(x, batch)\n        x_mean = global_mean_pool(x, batch)\n        x_max = global_max_pool(x, batch)\n        x = torch.cat([x_add, x_mean, x_max], dim=-1)\n        return self.out(x)\n\nclass GraphGAT(nn.Module):\n    def __init__(self, in_channels, hidden_dim, num_layers, out_channels, dropout, act):\n        super(GraphGAT, self).__init__()\n        self.tag = 'GraphGAT'\n        self.pre_embs = nn.Embedding(in_channels, hidden_dim)\n\n        self.convs = nn.ModuleList()\n        self.selus = nn.ModuleList()\n        self.dropouts = nn.ModuleList()\n        self.layer_norms = nn.ModuleList()\n\n        # Input layer\n        self.convs.append(GATConv(in_channels, hidden_dim))\n        self.selus.append(nn.SELU())\n        self.dropouts.append(nn.Dropout(dropout))\n        self.layer_norms.append(nn.LayerNorm(hidden_dim))\n\n        # Hidden layers\n        for _ in range(num_layers):\n            self.convs.append(GATConv(hidden_dim, hidden_dim))\n            self.selus.append(SELU())\n            self.dropouts.append(nn.Dropout(dropout))\n            self.layer_norms.append(nn.LayerNorm(hidden_dim))\n\n        # Output layer\n        self.convs.append(GATConv(hidden_dim, hidden_dim))\n        self.out = nn.Linear(3*hidden_dim, out_channels)\n        self.out.weight.data.fill_(1.)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, data):\n        \"\"\"Runs a forward pass through GraphGAT with given initial IDs and\n        edge_index and edge_weights.\n\n        Arguments:\n          - x: amino acid IDs (torch.Tensor)\n          - edge_index: edges in skill graph (torch.Tensor)\n          - edge_weight: edge weights of skill graph (torch.Tensor)\n\n        Returns:\n          - final node embedding for skill\n        \"\"\"\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        # Convert x tensor to Long type\n        #x = x.to(torch.long)\n        #x = self.pre_embs(x)\n        for conv, selu, dropout, layer_norm in zip(self.convs, self.selus, self.dropouts, self.layer_norms):\n          x = layer_norm(dropout(selu(conv(x, edge_index))))\n\n        x_add = global_add_pool(x, batch)\n        x_mean = global_mean_pool(x, batch)\n        x_max = global_max_pool(x, batch)\n        x = torch.cat([x_add, x_mean, x_max], dim=-1)\n        return self.out(x)\n","metadata":{"scrolled":true,"id":"5lrHQvSH1K4l","execution":{"iopub.status.busy":"2023-07-20T01:32:39.108286Z","iopub.execute_input":"2023-07-20T01:32:39.108767Z","iopub.status.idle":"2023-07-20T01:32:39.132372Z","shell.execute_reply.started":"2023-07-20T01:32:39.108710Z","shell.execute_reply":"2023-07-20T01:32:39.131199Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Here, **hidden_dim** is the dimensionality of hidden layers, and **output_dim** is the number of output classes (1500 in this case).\n\nNow, let's define a function to convert each NetworkX graph into a PyTorch Geometric **Data** object:","metadata":{"id":"BwaU1rn01K4n"}},{"cell_type":"code","source":"# Assuming nx_graphs_train and nx_graphs_test are your lists of NetworkX graphs\n# and labels_train and labels_test are your lists of labels for train and test datasets respectively\n#TODO Use map for this and do it with singles\ndef from_networkx_to_data(nx_graph, labels):\n    data = from_networkx(nx_graph)\n\n    # Extract node properties and convert them to a feature tensor\n    data.x = torch.tensor([[float(nx_graph.nodes[node][prop]) for prop in nx_graph.nodes[node]]\n                            for node in nx_graph.nodes()], dtype=torch.float)\n    \n    data.y = torch.tensor(labels, dtype=torch.float).unsqueeze(0)\n    return data\n    \ndef from_networkx_to_data_list(nx_graphs, labels):\n    return list(map(from_networkx_to_data, nx_graphs,labels))","metadata":{"id":"qTOl5aMC1K4n","execution":{"iopub.status.busy":"2023-07-20T01:32:39.136109Z","iopub.execute_input":"2023-07-20T01:32:39.136433Z","iopub.status.idle":"2023-07-20T01:32:39.146436Z","shell.execute_reply.started":"2023-07-20T01:32:39.136408Z","shell.execute_reply":"2023-07-20T01:32:39.145426Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"In this function, we convert the adjacency matrix of the graph into the edge index format expected by PyTorch Geometric. We also convert the labels into a torch tensor.\n\n# Training\n\nNow, let's define the training loop and train the GNN model on your labeled graphs:","metadata":{"id":"FpwTzGAl1K4o"}},{"cell_type":"code","source":"import random\nfrom datetime import datetime\nrandom.seed(datetime.now().timestamp())\nif notebook_run:\n    # Assuming you have a list of NetworkX graphs and corresponding labels\n\n    #protein_ids = list(protein_rn_dict.keys())\n    protein_ids = g_train_protein_ids\n    num_features = 33\n    num_classes = 1500\n    graphs, graphs_test, labels, labels_test = train_test_split([protein_rn_dict[k] for k in protein_ids],\n                                                                labels_df.iloc[np.where(np.isin(g_train_protein_ids, protein_ids))].values,\n                                                                test_size=0.2, random_state=int(datetime.now().timestamp()))\n\n    # Convert each graph to a PyTorch Geometric Data object and place in data loader\n    train_loader = DataLoader(from_networkx_to_data_list(graphs, labels), batch_size=256, shuffle=True)\n    test_loader = DataLoader(from_networkx_to_data_list(graphs_test, labels_test), batch_size=256, shuffle=False)","metadata":{"id":"VBPSIOJm1K4o","colab":{"base_uri":"https://localhost:8080/"},"outputId":"134e042a-31b8-4537-8157-7a576463b3ef","execution":{"iopub.status.busy":"2023-07-20T01:32:39.148231Z","iopub.execute_input":"2023-07-20T01:32:39.148597Z","iopub.status.idle":"2023-07-20T01:32:39.157832Z","shell.execute_reply.started":"2023-07-20T01:32:39.148563Z","shell.execute_reply":"2023-07-20T01:32:39.156923Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"We'll introduce a method to do all of this data preparation at once, such that we don't store all of these intermediate variables in memory.","metadata":{}},{"cell_type":"code","source":"class ProteinDataset(Dataset):\n    def __init__(self, protein_sequences, labels=torch.zeros((1500,1)), training=False):\n        self.protein_sequences = protein_sequences\n        self.labels = labels\n        self.training = training\n        if training:\n            self.dataset = list(map(from_networkx_to_data, map(protein_recurrence_network, protein_sequences), self.labels))\n\n    def __len__(self):\n        return len(self.protein_sequences)\n\n    def __getitem__(self, idx):\n        \n        # Convert the NetworkX graph to a PyTorch Geometric Data object\n        if self.training:\n            data = self.dataset[idx]\n        else:\n            data = from_networkx_to_data(protein_recurrence_network(self.protein_sequences[idx]), self.labels[idx])\n        return data\n\ndef prepare_training_data_and_models(sample_data_size=150000, num_of_labels=1500, training=True, just_data=False):\n    '''\n        Inputs:\n            sample_data_size - How many samples should be used in the training set\n            num_of_labels - Limit of how many lables (most frequently occurring) should we include\n    '''\n    train_terms = pd.read_csv(kaggle_input_data + \"/Train/train_terms.tsv\",sep=\"\\t\")\n    print(\"Training terms shape: \")\n    print(train_terms.shape)\n    if sample_data_size:\n        train_terms = train_terms.sample(sample_data_size)\n\n    print(\"Getting record IDs\")\n    # Parse the fasta file\n    records = SeqIO.parse(kaggle_input_data + \"/Train/train_sequences.fasta\", \"fasta\")\n    entry_id_set = set(train_terms['EntryID'].values)\n    train_protein_ids = np.array([record.id for record in records if record.id in entry_id_set])\n    print(str(len(train_protein_ids)) + \" Protein IDs found\")\n\n    print(\"Fetching Label Values\")\n    # Take value counts in descending order and fetch first 1500 `GO term ID` as labels\n    labels = train_terms['term'].value_counts().index[:num_of_labels].tolist()\n    \n    # Fetch the train_terms data for the relevant labels only\n    train_terms_updated = train_terms.loc[train_terms['term'].isin(labels)]\n    \n    print(\"Establishing labels\")\n    \n    # Setup progressbar settings.\n    # This is strictly for aesthetic.\n    bar = progressbar.ProgressBar(maxval=num_of_labels, \\\n        widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n\n    # Create an empty dataframe of required size for storing the labels,\n    # i.e, train_size x num_of_labels (142246 x 1500)\n    train_size = train_protein_ids.shape[0] # len(X)\n    train_labels = np.zeros((train_size, num_of_labels))\n\n    # Convert from numpy to pandas series for better handling\n    series_train_protein_ids = pd.Series(train_protein_ids)\n\n    # Generate a dict where key is label and value is the corresponding proteins\n    def get_label_proteins(label):\n        proteins = train_terms_updated[train_terms_updated['term'] == label]['EntryID'].unique()\n        return label, proteins\n    \n    print(\"Creating Label-Protein Dictionary\")\n\n    label_protein_dict = dict(map(get_label_proteins, labels))\n\n    # Loop through each label\n    for i, label in enumerate(labels):\n        # Fetch all the unique EntryId aka proteins related to the current label(GO term ID)\n        label_related_proteins = label_protein_dict[label]\n\n        # In the series_train_protein_ids pandas series, if a protein is related\n        # to the current label, then mark it as 1, else 0.\n        # Replace the ith column of train_labels with that pandas series.\n        train_labels[:,i] =  series_train_protein_ids.isin(label_related_proteins).astype(float)\n\n        # Progress bar percentage increase\n        bar.update(i+1)\n\n    # Notify the end of progress bar\n    bar.finish()\n\n    # Convert train_labels numpy into pandas dataframe\n    labels_df = pd.DataFrame(data = train_labels, columns = labels)\n\n    #protein_ids = list(protein_rn_dict.keys())\n    protein_ids = train_protein_ids\n    num_features = 33\n    num_classes = num_of_labels\n    proteins_train, proteins_test, labels, labels_test = train_test_split(protein_ids,\n                                                                labels_df.iloc[np.where(np.isin(train_protein_ids, protein_ids))].values,\n                                                                test_size=0.2, random_state=int(datetime.now().timestamp()))\n    print(\"Getting sequences for training\")\n    records = SeqIO.parse(kaggle_input_data + \"/Train/train_sequences.fasta\", \"fasta\")\n    proteins_train_set = set(proteins_train)\n    training_dataset = ProteinDataset([str(record.seq) for record in records if record.id in proteins_train_set], labels, training=training)\n    print(\"Getting sequences for testing\")\n    records = SeqIO.parse(kaggle_input_data + \"/Train/train_sequences.fasta\", \"fasta\")\n    proteins_test_set = set(proteins_test)\n    test_dataset = ProteinDataset([str(record.seq) for record in records if record.id in proteins_test_set], labels_test, training=training)\n    \n    print(\"Putting training data into Data Loader\")\n    # Convert each graph to a PyTorch Geometric Data object and place in data loader\n    train_loader = DataLoader(training_dataset, batch_size=512, shuffle=True, num_workers=2, pin_memory=True)\n    print(\"Putting test data into Data Loader\")\n    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n    \n    # Prepare Model\n    print(\"Preparing model\")\n    def weights_init(m):\n        if isinstance(m, nn.Linear):\n            m.weight.data.fill_(1.)\n\n    if just_data:\n        return train_loader, test_loader\n    else:\n        # Calculate the class frequencies (number of occurrences) for each label\n        class_frequencies = labels_df.sum(axis=0)\n\n        # Calculate the total number of samples (rows)\n        total_samples = len(labels_df)\n\n        # Calculate the class weights as the inverse of class frequencies\n        class_weights = total_samples / (class_frequencies * len(labels_df.columns))\n\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        #model = GINNet(num_features, 256, num_classes).to(device)\n        model = GraphGAT(in_channels=num_features,hidden_dim=128,num_layers=5,out_channels=num_classes,dropout=0.1,act='selu').to(device)\n        #model = GraphSAGE(num_features, 128, num_classes).to(device)\n        # Initialize final layer to 0s\n        model.apply(weights_init)\n        hidden_channels = 64\n        num_layers = 30\n        #model = GCN(num_features, 256, num_classes).to(device)\n        #model = GraphUnet(num_features, num_classes, n_hidden=128)\n        #model = models.GraphSAGE(num_features,hidden_channels,num_layers,num_classes,dropout=0.1,act='selu',act_first=True).to(device)\n        return train_loader, test_loader, model, device, class_weights","metadata":{"execution":{"iopub.status.busy":"2023-07-20T01:45:14.661426Z","iopub.execute_input":"2023-07-20T01:45:14.661841Z","iopub.status.idle":"2023-07-20T01:45:14.692485Z","shell.execute_reply.started":"2023-07-20T01:45:14.661810Z","shell.execute_reply":"2023-07-20T01:45:14.691278Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"if not notebook_run:\n    train_loader, test_loader, model, device, class_weights = prepare_training_data_and_models(sample_data_size=10000)","metadata":{"execution":{"iopub.status.busy":"2023-07-20T01:45:14.694737Z","iopub.execute_input":"2023-07-20T01:45:14.695292Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Training terms shape: \n(5363863, 3)\nGetting record IDs\n9307 Protein IDs found\nFetching Label Values\nEstablishing labels\nCreating Label-Protein Dictionary\n","output_type":"stream"},{"name":"stderr","text":"[========================================================================] 100%\n","output_type":"stream"},{"name":"stdout","text":"Getting sequences for training\n","output_type":"stream"}]},{"cell_type":"code","source":"for i,data in enumerate(train_loader):\n    print(f'Step {i + 1}:')\n    print('=======')\n    print(f'Number of graphs in the current batch: {data.num_graphs}')\n    print(data)\n    print()\n    \n    if i > 10:\n        break","metadata":{"scrolled":true,"id":"oZkqGbCD1K4p","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f5e72260-62c8-4f50-e8b0-ce4a48944faa","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# After running each iteration:\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import clear_output\nfrom torchmetrics.classification import MultilabelStatScores, MultilabelAccuracy\nimport torch.nn.functional as F\nimport time\n\n\nif notebook_run:\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    #model = GINNet(num_features, 256, num_classes).to(device)\n    model = GraphSAGE(num_features, 256, num_classes).to(device)\n\n    hidden_channels = 64\n    num_layers = 30\n    #model = GCN(num_features, 256, num_classes).to(device)\n    #model = GraphUnet(num_features, num_classes, n_hidden=128)\n    #model = models.GraphSAGE(num_features,hidden_channels,num_layers,num_classes,dropout=0.1,act='selu',act_first=True).to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\nloss_function = torch.nn.MultiLabelSoftMarginLoss(weight = torch.Tensor(class_weights.values).to(device) )\naccuracy = MultilabelAccuracy(num_labels=1500).to(device)\nmlss = MultilabelStatScores(1500, average='micro').to(device)\n\ndef train():\n    model.train()\n\n    correct = 0\n    total_loss = 0\n    for i,data in enumerate(train_loader):\n        data = data.to(device)\n        out = model(data)\n        loss = loss_function(out, data.y)\n        loss.backward()  # Backward pass (calculate gradients)\n\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        total_loss += loss\n            \n    # After the training loop ends, there might be gradients that are not yet updated\n    # So, you should perform an optimization step outside the loop to update those gradients\n    optimizer.step()\n\n    return total_loss / len(train_loader.dataset)  # Derive ratio of correct predictions.\n\ndef test(data_loader, verbose=True):\n    model.eval()\n\n    #all_preds = []\n    #all_labels = []\n    total_acc = 0\n    clear_output(wait=True)\n\n    with torch.no_grad():\n        for data in data_loader:\n            data = data.to(device)\n            out = model(data)\n            total_acc += accuracy(out,data.y)\n            if verbose:\n                print('[tp, fp, tn, fn, sup]')\n                print(mlss(out,data.y))\n    #if verbose:\n    #    print(classification_report(all_labels, all_preds))\n    return total_acc / len(data_loader.dataset)  # Derive ratio of correct predictions.\n\n    \nnum_folds = 500\nnum_epochs = 100\nlosses = []\ntest_accs = []\nepochs = []\n\nfor k in range(num_folds):\n    print(\"Fold: \" + str(k+1))\n    for epoch in range(1, num_epochs):\n        start = time.time()\n        loss = train().detach().cpu()\n        test_acc = test(test_loader).detach().cpu()\n        end = time.time()\n    \n        # Update the loss, test_acc, and epochs variables\n        losses.append(loss)\n        test_accs.append(test_acc)\n        epochs.append(epoch)\n\n        # Plot the updated data\n        plt.plot(epochs, losses, label='Loss')\n        plt.plot(epochs, test_accs, label='Test Accuracy')\n\n        # Add labels and a legend to the plot\n        plt.xlabel('Epoch')\n        plt.ylabel('Value')\n        plt.legend()\n\n        # Label the last loss and test_acc values\n        last_loss = round(losses[-1].numpy().tolist(), 4)\n        last_acc = round(test_accs[-1].numpy().tolist(), 4)\n        plt.text(epochs[-1], losses[-1], f'{last_loss}', ha='right', va='bottom')\n        plt.text(epochs[-1], test_accs[-1], f'{last_acc}', ha='right', va='bottom')\n\n        print(f'Epoch: {epoch:03d}, Train Loss: {loss:e}, Test Acc: {test_acc:e}, Time: {end-start:.4f}')\n        # Show the plot\n        plt.show()\n    \n    train_loader, test_loader = prepare_training_data_and_models(sample_data_size=10000, just_data=True)\n\ntorch.save(model, 'gcm.torch')\ndel train_loader\ndel test_loader","metadata":{"id":"AykYOE5u1K4p","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d782a1f0-728a-415e-ca0d-1313c4b42760","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate\n\nNow, we write some basic code to take a Networkx graph and predict the classes.","metadata":{"id":"jmEIO7Bj1K4r"}},{"cell_type":"code","source":"def predict_mult(ids, sequences, verbose=False):    \n    protein_dataset = ProteinDataset(sequences, torch.zeros((len(ids),1500,1)), training=False)\n    loader = DataLoader(protein_dataset, batch_size=1024, shuffle=False)\n\n    if verbose:\n        print(\"Evaluating sequences\")\n        print(\"Total sequences: \" + str(len(ids)))\n        \n    # Set the model to evaluation mode\n    model.eval()\n    predictions = []  # initialize an empty list to collect all predictions\n    \n    # Forward pass through the model\n    with torch.no_grad():\n        \n        for i,batch in enumerate(loader):\n            batch = batch.to(device)  # move batch to the device (GPU or CPU)\n            outputs = model(batch)  # forward pass\n            # process the outputs (e.g., apply a threshold in case of binary classification, etc.)\n            batch_predictions = (outputs > 0.5).float()\n            if verbose:\n                print(\"Batch output \" + str(i) + \": \")\n                print(batch_predictions)\n            # Detach the predictions from the computation graph and convert to a numpy array.\n            # Then add them to the list of all predictions.\n            predictions.extend(batch_predictions.cpu().numpy().tolist())\n\n        if verbose:\n            print(\"Model prediction: \")\n            print(predictions)\n        #output = model(x=data.x, edge_index=data.edge_index, edge_weight=edge_weight)\n\n    return ids, predictions\n\ndef predict(sequence, verbose=False):\n    g = protein_recurrence_network(sequence)\n    # Convert the NetworkX graph to a PyTorch Geometric Data object\n    data = from_networkx_to_data(g, torch.zeros((1500,1)))\n    \n    if verbose:\n        print(\"Data: \")\n        print(data)\n\n    # Move data to the appropriate device\n    data = data.to(device)\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Forward pass through the model\n    with torch.no_grad():\n        output = model(data)\n        if verbose:\n            print(\"Model prediction: \")\n            print(output)\n        #output = model(x=data.x, edge_index=data.edge_index, edge_weight=edge_weight)\n\n    # The output is log-probabilities, use softmax to get probabilities\n    out_probs = torch.sigmoid(output)\n    \n    # In a multi-label classification, an element is considered to be predicted as '1'\n    # if its probability is greater than or equal to 0.5.\n    out_preds = (out_probs > 0.5).float()\n\n    return out_preds","metadata":{"id":"x6g52g-i1K4s","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_seq = 'YAWVHDHCNWCLERVGTVHDEKWTMEDPMVPKHCECNEPIQAWTQDNDLYNFCITLQICANNNRDGGNLIGRVDQLALQKRLLDQNWHKNCTPPSVVTRCNCATCEEKVETRVHFMKIMGESGWDGWVMYLPLGQQICYSIPHRQHKCWWSIWFVDFRLLPIGERNDTLSCFMIIKLEWIFDVCHLHDKSIEIAEAGVQIQVWAQQICSGTYIEGWLDWENSPIACPDGDYWSNWTAMVVAFCECRKRMCSVIPQTKFYAFHMVHNKWWLQWFYTSHEGKKIVGIEYHQGSCYYKWTKGEKHMHMNVEQRQWGADQVVHTPFNAWACWMHTAKHGDCNVPGQHWGMGWFRDDDELAAYGHHTEHGDATQLLFGTCYVCPLNIVYLMGTLVLPRHQHVRNKRPMVDRDMVYTRDKIIQELVWHGSYYILRPLMMRNTQHIKYLVYRGFHSIPKKDIAQPKRRGNKVGDKHVWKQFWIGSQPNHSKNTLMDLWHSAFMKWCNDFPDEPTKYVAGWPNIHPMGLALCLHPPSPRREKWDKHMFTPDYDHMSSWPEFAAHMDDCWVHNTFQIPWFWPNHHGRFHAQNVFLVWFTTGIKAGLDLMNICMIWGDKPRTHKIIFHHMRDWKNHPIEFTMKMEEKYHWDGKFDYHLPFRYWDRMSIRHDNYKTSWPCGWPSWHYALIELTCYGEMEIYGWADQEYRQCDQTFHMQLQMQNMTKLIRWYTHLGCDSVMCVQSALHTLKEWKRPTRMIKNGLGLILNLMVMACAGHFTISDGFLLLESPWKWGNSYSGNSVEGILAVKGVDYDFDGDYAWEQRSNQQWVGGLMQCILAGLPRLNLEMFRQESMWANTYGMPSYKQTDFFHSNLIRTKRMDEMSHAMVRGWTGILVNFTHHNWTWYMSLLCQSAYGTARTIFTGGNPAADDNRDLDHEDDEVYDVWIDSECTAQWLSMPVGIPNYGFCTCQAKQWINRGIPAGKLFMEVCPWMNDNQKGASSHCYIIEWCS'\nt = predict(test_seq)\ndisplay(t)\ntest_seqs = ['YAWVHDHCNWCLERVGTVHDEKWTMEDPMVPKHCECNEPIQAWTQDNDLYNFCITLQICANNNRDGGNLIGRVDQLALQKRLLDQNWHKNCTPPSVVTRCNCATCEEKVETRVHFMKIMGESGWDGWVMYLPLGQQICYSIPHRQHKCWWSIWFVDFRLLPIGERNDTLSCFMIIKLEWIFDVCHLHDKSIEIAEAGVQIQVWAQQICSGTYIEGWLDWENSPIACPDGDYWSNWTAMVVAFCECRKRMCSVIPQTKFYAFHMVHNKWWLQWFYTSHEGKKIVGIEYHQGSCYYKWTKGEKHMHMNVEQRQWGADQVVHTPFNAWACWMHTAKHGDCNVPGQHWGMGWFRDDDELAAYGHHTEHGDATQLLFGTCYVCPLNIVYLMGTLVLPRHQHVRNKRPMVDRDMVYTRDKIIQELVWHGSYYILRPLMMRNTQHIKYLVYRGFHSIPKKDIAQPKRRGNKVGDKHVWKQFWIGSQPNHSKNTLMDLWHSAFMKWCNDFPDEPTKYVAGWPNIHPMGLALCLHPPSPRREKWDKHMFTPDYDHMSSWPEFAAHMDDCWVHNTFQIPWFWPNHHGRFHAQNVFLVWFTTGIKAGLDLMNICMIWGDKPRTHKIIFHHMRDWKNHPIEFTMKMEEKYHWDGKFDYHLPFRYWDRMSIRHDNYKTSWPCGWPSWHYALIELTCYGEMEIYGWADQEYRQCDQTFHMQLQMQNMTKLIRWYTHLGCDSVMCVQSALHTLKEWKRPTRMIKNGLGLILNLMVMACAGHFTISDGFLLLESPWKWGNSYSGNSVEGILAVKGVDYDFDGDYAWEQRSNQQWVGGLMQCILAGLPRLNLEMFRQESMWANTYGMPSYKQTDFFHSNLIRTKRMDEMSHAMVRGWTGILVNFTHHNWTWYMSLLCQSAYGTARTIFTGGNPAADDNRDLDHEDDEVYDVWIDSECTAQWLSMPVGIPNYGFCTCQAKQWINRGIPAGKLFMEVCPWMNDNQKGASSHCYIIEWCS','YAWVHDHCNVPKHCECNEPIQAWTQDNDLYNFCITLQICANNNRDGGNLIGRVDQLALQKRLLDQNWHKNCTPPSVVTRCNCATCEEKVETRVHFMKIMGESGWDGWVMYLPLGQQICYSIPHRQHKCWWSIWFVDFRLLPIGERNDTLSCFMIIKLEWIFDVCHLHDKSIEIAEAGVQIQVWAQQICSGTYIEGWLDWENSPIACPDGDYWSNWTAMVVAFCECRKRMCSVIPQTKFYAFHMVHNKWWLQWFYTSHEGKKIVGIEYHQGSCYYKWTKGEKHMHMNVEQRQWGADQVVHTPFNAWACWMHTAKHGDCNVPGQHWGMGWFRDDDELAAYGHHTEHGDATQLLFGTCYVCPLNIVYLMGTLVLPRHQHVRNKRPMVDRDMVYTRDKIIQELVWHGSYYILRPLMMRNTQHIKYLVYRGFHSIPKKDIAQPKRRGNKVGDKHVWKQFWIGSQPNHSKNTLMDLWHSAFMKWCNDFPDEPTKYVAGWPNIHPMGLALCLHPPSPRREKWDKHMFTPDYDHMSSWPEFAAHMDDCWVHNTFQIPWFWPNHHGRFHAQNVFLVWFTTGIKAGLDLMNICMIWGDKPRTHKIIFHHMRDWKNHPIEFTMKMEEKYHWDGKFDYHLPFRYWDRMSIRHDNYKTSWPCGWPSWHYALIELTCYGEMEIYGWADQEYRQCDQTFHMQLQMQNMTKLIRWYTHLGCDSVMCVQSALHTLKEWKRPTRMIKNGLGLILNLMVMACAGHFTISDGFLLLESPWKWGNSYSGNSVEGILAVKGVDYDFDGDYAWEQRSNQQWVGGLMQCILAGLPRLNLEMFRQESMWANTYGMPSYKQTDFFHSNLIRTKRMDEMSHAMVRGWTGILVNFTHHNWTWYMSLLCQSAYGTARTIFTGGNPAADDNRDLDHEDDEVYDVWIDSECTAQWLSMPVGIPNYGFCTCQAKQWINRGIPAGKLFMEVCPWMNDNQKGASSHCYIIEWCS']\nids,ts = predict_mult(['t1','t2'],test_seqs)\ndisplay(ts)\n#torch.nonzero(predict(test_seq))\n#labels_df.iloc[np.where(train_protein_ids=='P20536')[0][0]]","metadata":{"id":"t73eH9Tn1K4t","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.array(ts)","metadata":{"id":"7jqhlXTy1K4u","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t","metadata":{"id":"jpTtjiDO1K4v","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{"papermill":{"duration":0.021665,"end_time":"2023-05-09T08:41:01.780867","exception":false,"start_time":"2023-05-09T08:41:01.759202","status":"completed"},"tags":[],"id":"NsMJWTki1K4w"}},{"cell_type":"markdown","source":"For submission we will use the protein embeddings of the test data created by [Sergei Fironov](https://www.kaggle.com/sergeifironov) using the Rost Lab's T5 protein language model.","metadata":{"papermill":{"duration":0.02075,"end_time":"2023-05-09T08:41:01.82296","exception":false,"start_time":"2023-05-09T08:41:01.80221","status":"completed"},"tags":[],"id":"zRulLIuJ1K4x"}},{"cell_type":"code","source":"# Parse the fasta file\nkaggle_input_embeds = '/kaggle/input/t5embeds'\ntest_protein_ids = set(np.load(kaggle_input_embeds + '/test_ids.npy'))\ntest_records = SeqIO.parse(kaggle_input_data + '/Test (Targets)/testsuperset.fasta', format='fasta')\n\npred_ids, predictions = predict_mult(*list(zip(*[(record.id, str(record.seq)) for record in test_records if record.id in test_protein_ids[:100]])), verbose=True)","metadata":{"papermill":{"duration":10.290827,"end_time":"2023-05-09T08:41:12.134919","exception":false,"start_time":"2023-05-09T08:41:01.844092","status":"completed"},"tags":[],"id":"NnNy1w7z1K4x","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"papermill":{"duration":663.907351,"end_time":"2023-05-09T08:52:16.178461","exception":false,"start_time":"2023-05-09T08:41:12.27111","status":"completed"},"tags":[],"id":"QaYA0mN71K41","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the predictions we will create the submission data frame.\n\n**Note**: This will take atleast **15 to 20** minutes to finish.","metadata":{"id":"7EgfXYPY1K41"}},{"cell_type":"code","source":"# Reference: https://www.kaggle.com/code/alexandervc/baseline-multilabel-to-multitarget-binary\n\ndf_submission = pd.DataFrame(columns = ['Protein Id', 'GO Term Id','Prediction'])\nl = []\nfor k in list(test_protein_ids):\n    l += [ k] * predictions.shape[1]\n\ndf_submission['Protein Id'] = l\ndf_submission['GO Term Id'] = labels * predictions.shape[0]\ndf_submission['Prediction'] = predictions.ravel()\ndf_submission.to_csv(\"submission.tsv\",header=False, index=False, sep=\"\\t\")","metadata":{"id":"kPp6NFlC1K42","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_submission","metadata":{"papermill":{"duration":0.063739,"end_time":"2023-05-09T08:52:16.292974","exception":false,"start_time":"2023-05-09T08:52:16.229235","status":"completed"},"tags":[],"id":"-ZcOhlK61K42","trusted":true},"execution_count":null,"outputs":[]}]}